---
output: pdf_document
header-includes:
- \usepackage{booktabs}
- \usepackage{sectsty} \allsectionsfont{\centering}
- \usepackage{indentfirst}
urlcolor: blue
indent: true
---

\noindent Matt Alvarez-Nissen and Lilla Petruska

\noindent MS&E 226

\noindent `r format(Sys.time(), "%m/%d/%Y")`

\allsectionsfont{\centering} 
## MS&E 226 Project Part 2 - Neighborhood Outages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Please exclude this page from page count
# Libraries
library(tidyverse)
library(caret)
library(knitr)
library(kableExtra)
library(boot)

# Load Data
acs_outages_train <- 
  read_csv(paste0(here::here(), "/cleaned_data/acs_outages_train.csv"))
acs_outages_test <-
  read_csv(paste0(here::here(), "/cleaned_data/acs_outages_test.csv"))

# Parameters
lambdas <- 10^seq(3, -2, by = -.1)
## List of outcome variables
outcome_vars <- c("median_outage_duration_hr", "above_median_cust_affected")

## List of covariates
continuous_vars <-
  names(acs_outages_train %>% select(-c(all_of(outcome_vars), GEOID)))

# Functions
eval_results <- function(model, true, predicted) {
  model <- enquo(model)
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/length(true))

  # Model performance metrics
  data.frame(
    model = as_label(model),
    RMSE = RMSE,
    Rsquare = R_square
  )
}
```

```{r, echo=FALSE}
# Regression df
reg_train <-
  acs_outages_train %>%
  select(-c(GEOID, above_median_cust_affected, prop_white, prop_eli, prop_college, prop_owner, rental_vacancy_rate, prop_rural)) %>%
  drop_na()

reg_test <- 
  acs_outages_test %>% 
  select(-c(GEOID, above_median_cust_affected, prop_white, prop_eli, prop_college, prop_owner, rental_vacancy_rate, prop_rural)) %>%
  drop_na()

# Classification df
# Set up data for classification (and drop NAs)
acs_outages_class_train <- 
  acs_outages_train %>% 
  select(
    -c(GEOID, median_outage_duration_hr)
  ) %>% 
  drop_na() 

# Set up data for classification (and drop NAs)
acs_outages_class_test <- 
  acs_outages_test %>% 
  select(
    -c(GEOID, median_outage_duration_hr)
  ) %>% 
  drop_na() 

# Transform covariates for linear regression
reg_train_transform <-
  reg_train %>%
   mutate(
    prop_latino = log10(prop_latino + 1),
    prop_less_than_hs = log10(prop_less_than_hs + 1)
  )

reg_test_transform <-
  reg_test %>%
   mutate(
    prop_latino = log10(prop_latino + 1),
    prop_less_than_hs = log10(prop_less_than_hs + 1)
  )


# following code is unncessary (for now)
# # Logistic boot model df
# acs_outages_log_train <-
#   acs_outages_train %>% 
#   select(
#     -c(GEOID, median_outage_duration_hr, prop_white, prop_eli, 
#        prop_college, prop_owner, rental_vacancy_rate, prop_rural)
#   ) %>% 
#   drop_na() 
# 
# acs_outages_log_test <-
#   acs_outages_test %>% 
#   select(
#     -c(GEOID, median_outage_duration_hr, prop_white, prop_eli, 
#        prop_college, prop_owner, rental_vacancy_rate, prop_rural)
#   ) %>% 
#   drop_na() 
```

# Prediction on the test set
## Regression

```{r, ech0=FALSE}
lr_transform <- lm(median_outage_duration_hr ~ ., data = reg_train_transform)

transform_predict <- predict(lr_transform, reg_test_transform)
RMSE(reg_test_transform$median_outage_duration_hr, transform_predict)
```

## Classification
```{r, echo=FALSE}
# conduct PCA (to reduce data, especially collinearity)
# Find variance of each covariate
train_col_var <- 
  acs_outages_class_train %>% 
  select(-above_median_cust_affected) %>% 
  mutate(across(everything(), stats::var)) %>% 
  distinct() %>% 
  pivot_longer(cols = everything(), names_to = "vars", values_to = "variance") %>% 
  # filter out 0 variance covariates
  filter(variance != 0)

# remove zero variance covariates
class_filter <-
  acs_outages_class_train %>% 
  # remove outcome var as well
  select(all_of(train_col_var$vars))

# Run PCA
pca <- prcomp(class_filter, scale = T, center = T)
```

```{r, echo=FALSE}
# PCA results
pca_table_train <- 
  data.frame(
    above_median_cust_affected = acs_outages_class_train[,1] %>% pull(), pca$x
  ) %>% 
  #filter to the first 7 components
  select(above_median_cust_affected:PC7) %>% 
  bind_cols()
```

```{r,echo=FALSE}
# Do 10-fold CV KNN
set.seed(243)
# Set train control
knn_control <- 
  trainControl(
    method = "cv", 
    number = 10
  )

# Evaluate accuracy of KNN classifiers
knn_fit <-
  train(
    as.factor(above_median_cust_affected) ~ .,
    method = "knn",
    # Limit to k = sqrt(n)
    tuneGrid = expand.grid(k = 1:sqrt(nrow(pca_table_train))),
    trControl = knn_control,
    metric = "Accuracy",
    data = pca_table_train
  )
```

```{r, echo=FALSE}
# create confusion matrix
confusion_matrix_knn <- confusionMatrix(knn_fit, positive = 1)
confusion_matrix_knn <- confusion_matrix_knn$table %>% t()
```

```{r, echo=FALSE}
# calculate confusion matrix scores
tn <- confusion_matrix_knn[1,1]
fn <- confusion_matrix_knn[2,1]
fp <- confusion_matrix_knn[1,2]
tp <- confusion_matrix_knn[2,2]

train_01_loss <- (fp + fn) / nrow(acs_outages_class_train)
train_sensitivity <- tp / (fn + tp)
train_specificity <- tn / (tn + fp)
train_precision <- tp / (tp + fp)
train_typeI_error <- fp / (tn + fp)
train_typeII_error <- fn / (fn + tp)
train_false_discovery <- fp / (tp + fp)
```

```{r, echo=FALSE}
# conduct PCA (to reduce data, especially collinearity)
# Find variance of each covariate
test_col_var <- 
  acs_outages_class_test %>% 
  select(-above_median_cust_affected) %>% 
  mutate(across(everything(), stats::var)) %>% 
  distinct() %>% 
  pivot_longer(cols = everything(), names_to = "vars", values_to = "variance") %>% 
  # filter out 0 variance covariates
  filter(variance != 0)

# remove zero variance covariates
class_filter <-
  acs_outages_class_test %>% 
  # remove outcome var as well
  select(all_of(test_col_var$vars))

# Run PCA
pca <- prcomp(class_filter, scale = T, center = T)
```

```{r, echo=FALSE}
# PCA results
pca_table_test <- 
  data.frame(
    above_median_cust_affected = acs_outages_class_test[,1] %>% pull(), pca$x
  ) %>% 
  #filter to the first 7 components
  select(above_median_cust_affected:PC7) %>% 
  bind_cols()
```

```{r,echo=FALSE}
# Do 10-fold CV KNN
set.seed(243)
# Set train control
knn_control <- 
  trainControl(
    method = "cv", 
    number = 10
  )

# Evaluate accuracy of KNN classifiers
knn_fit <-
  train(
    as.factor(above_median_cust_affected) ~ .,
    method = "knn",
    # Limit to k = sqrt(n)
    tuneGrid = expand.grid(k = 1:sqrt(nrow(pca_table_test))),
    trControl = knn_control,
    metric = "Accuracy",
    data = pca_table_test
  )
```

```{r, echo=FALSE}
# create confusion matrix
confusion_matrix_knn <- confusionMatrix(knn_fit, positive = 1)
confusion_matrix_knn <- confusion_matrix_knn$table %>% t()
```

```{r, echo=FALSE}
# calculate confusion matrix scores
tn <- confusion_matrix_knn[1,1]
fn <- confusion_matrix_knn[2,1]
fp <- confusion_matrix_knn[1,2]
tp <- confusion_matrix_knn[2,2]

test_01_loss <- (fp + fn) / nrow(acs_outages_class_test)
test_sensitivity <- tp / (fn + tp)
test_specificity <- tn / (tn + fp)
test_precision <- tp / (tp + fp)
test_typeI_error <- fp / (tn + fp)
test_typeII_error <- fn / (fn + tp)
test_false_discovery <- fp / (tp + fp)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# confusion matrix comparison
confusion_matrix_comparison <-
  tibble(
    metric =
      c("Mean 0-1 Loss", "Precision", "Sensitivity", "Specificity",
        "Type I Error Rate", "Type II Error Rate", "False Discovery Rate"),
    train = 
      c(train_01_loss, train_precision, train_sensitivity, train_specificity,
        train_typeI_error, train_typeII_error, train_false_discovery),
    test = 
      c(test_01_loss, test_precision, test_sensitivity, test_specificity,
        test_typeI_error, test_typeII_error, test_false_discovery)
  ) %>% 
  kable(caption = "Above Median Customer Affected Confusion Matrix Metrics") %>% 
  kableExtra::kable_classic() %>% 
  #format for markdown
  kable_styling(latex_options="scale_down")
confusion_matrix_comparison
```

Mean 0-1 loss increased significantly and sensitivity dropped. Other metrics improved, like precision, while others worsened, like the Type II Error Rate. Given our focus on 0-1 loss and sensitivity, this model had an underwhelming performance. It would have been preferable to maintain a higher sensitivity rate with a lower 0-1 loss, but it is not clear how possible this is due to the low explanatory nature of the covariates.

# Inference
## Parts a) and b) below
```{r}
# variable selection (non-PCA)
# start with basic logistic model (train)
logit_mod_train <-
  glm(
    above_median_cust_affected ~ .,
    data = acs_outages_class_train, 
    family = binomial
  ) %>% 
  # stepwise selection by AIC
  MASS::stepAIC(trace = FALSE, direction = "both")
summary(logit_mod_train)

# extract just the selected variables to run through the bootstrap
acs_outage_var_select <-
  acs_outages_class_train %>% 
  select(above_median_cust_affected, any_of(names(logit_mod_train$coefficients)))

```

# Ignore the following two chunks
```{r}
# # start with basic logistic model (train)
# logit_mod_train <-
#   glm(
#     above_median_cust_affected ~ .,
#     data = acs_outages_log_train, 
#     family = binomial
#   )
# summary(logit_mod_train)

# PCA logistic model (train) - NOT SURE IF WE SHOULD USE
# pca_logit_mod_train <-
#   glm(
#     above_median_cust_affected ~ .,
#     data = pca_table_train, 
#     family = binomial
#   )
# summary(pca_logit_mod_train)
```

```{r}
# # start with basic logistic model (test)
# logit_mod_test <-
#   glm(
#     above_median_cust_affected ~ .,
#     data = acs_outages_log_test, 
#     family = binomial
#   )
# summary(logit_mod_test)

# PCA logistic model (test) - NOT SURE IF WE SHOULD USE
# pca_logit_mod_test <-
#   glm(
#     above_median_cust_affected ~ .,
#     data = pca_table_test,
#     family = binomial
#   )
# summary(pca_logit_mod_train)
```

## Part c)
```{r}
# bootstrap CI for each reg coefficient
# create function to return coefficients
boot_coefficients <- function(data, indices) {
  data <- data[indices,]
  
  logit_mod <-
    glm(
      above_median_cust_affected ~ .,
      # use reduced variables dataset
      data = data, 
      family = binomial
    )
  coefficients(logit_mod)
}

# Need to adjust R value (number of replicates, not sure what to use)
logit_boot <- boot(acs_outage_var_select, boot_coefficients, R = 2000)

# Loop through the results and create table of coefficient CIs
coef_ci_table <- c()
for (i in 1:ncol(acs_outage_var_select)) {
  # run the boot CI on each coefficient
  result <- boot.ci(logit_boot, index = i, type = "norm")
  coef_ci_table <- 
    result$normal %>% 
    as.data.frame() %>% 
    rename(lower_ci = V2, upper_ci = V3) %>% 
    rownames_to_column(var = "coefficient") %>% 
    bind_rows(coef_ci_table)
}

# merge CI table with original coefficient values
coef_comparison_table <-
  coefficients(logit_mod_train) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "coefficient") %>% 
  rename(value = ".") %>% 
  left_join(coef_ci_table, by = "coefficient")
coef_comparison_table %>% 
  kable(caption = "Coefficient Confidence Intervals") %>% 
  kableExtra::kable_classic() %>% 
  #format for markdown
  kable_styling(latex_options="scale_down")
```


# Discussion

a) How would you expect your models to be used practically? Do you think they would primarily
be used for prediction, for inference, or for both? What decisions do you think your
models would guide, and what pitfalls do you see in using your models to make these decisions?

While PG&E and other electric utilities monitor their outages and service quality through numerous quantitative metrics, including spatial distribution, there is little emphasis on the communities that are affected by these disruptions in service. Of course, power outages impact residential, commerical, and industrial customers in different ways. However, we must also consider the equity implications of who is being impacted by irregular electricity service. Our models aim to 

b) How well would your models hold up over time (i.e., how often do you think they should be
refitted)? Why?

Our models were fitted on power outage data from the last several months, but PG&E and other utilites are constantly collecting new data on their service regularity and variability. Patterns in the frequency and duration of power outages will likely change over time due to the prevalence of wildfires, improvements in transmission and distribution infrastructure, and increased ability to predict wildfire behavior and movement. While the United States Census Bureau performs the census every ten years, the American Community Survey releases new data yearly. Our demographic data is thus relatively up to date and provides a current snapshot of census tract demographics. Therefore, our model will need to be refitted regularly with the newest data on both outages and demographic data. 

** how to integrate changes in land use, satellite data 

c) Are there choices you made in your data analysis, that you would want to make sure any
one (e.g., a manager, a client, etc.) that uses your models is aware of? Examples here might
include approaches to data cleaning; data transformations that you chose; vulnerability to
overfitting, multiple hypothesis testing, or post-selection inference; etc.

Our 

d) If you could, how would you change the data collection process? In particular, are there
reasonable covariates you would like to collect, that were not present in the data?

PG&E only provides a single coordinate for each outage. Therefore there is not reliable information concerning the extent of the entire affected outage area. We georeferenced the outage coordinate to determine which census tract it corresponded to, though this only allowed us to infer what communities a specific power outage affected.

PG&E does not provide a data dictionary, so we must infer what the different variables mean based on their names. 

Because we only used PG&E outage data, our model was design to elucidate utility service quality in primarily Northern California census tracts. Our entire dataset was only 2722 rows, which is relatively small and may have impacted our ability to find meaningful insights. There are other major electric utilites in California, such as Southern California Edison and San Diego Gas and Electric, that also serve geographies severely impacted by wildfires and with socioeconomic disparities. By building a dataset that included outage data on all 8,057 California census tracts, we would have a more complete picture of electric utility service in California. This would also allow us to understand differences in service between utilities.

e) If you were to attack the same dataset again, what would you do differently?

When embarking on this topic, we believed that demographic characteristics of census tracts would have some predictive power to understand why PG&E's power outages were distributed the way they are. Historically, the quality of public services differ vastly based on communities' socioeconomic status, race, and wealth. After completing our analysis, there are likely many other factors at play in determining PG&E's ability to return power to its customers. Environmental and geographic factors, as well as PG&E's own behind-the-scenes decision making about where to cut power, will surely influence the prevalence and duration of power outages. Environmental and geographic factors that could have benefitted our model include the size, proximity, and presence of wildfire; quality of transmission and distribution infrastructure (including powerlines and voltage boxes); more detailed land cover data (vegetation, temperative, albedo, forested area); and building infrastructure data (industrial and commercial offtakers use more electricity than residential customers). Perhaps it would have been wiser to try to model how PG&E itself makes choices about where and how long to cut power for. 

On one hand, it is a good thing that PG&E's service does not seemingly discriminate against certain groups based on the proportion of that demographic indicator present in a census tract. 




