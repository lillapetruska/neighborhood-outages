---
output: pdf_document
header-includes:
- \usepackage{booktabs}
- \usepackage{sectsty} \allsectionsfont{\centering}
- \usepackage{indentfirst}
urlcolor: blue
indent: true
---

\noindent Matt Alvarez-Nissen and Lilla Petruska

\noindent MS&E 226

\noindent `r format(Sys.time(), "%m/%d/%Y")`

\allsectionsfont{\centering} 
## MS&E 226 Project Part 2 - Neighborhood Outages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Please exclude this page from page count
# Libraries
library(tidyverse)
library(caret)
library(knitr)
library(kableExtra)
library(boot)

# Load Data
acs_outages_train <- 
  read_csv(paste0(here::here(), "/cleaned_data/acs_outages_train.csv"))
acs_outages_test <-
  read_csv(paste0(here::here(), "/cleaned_data/acs_outages_test.csv"))

# Parameters
lambdas <- 10^seq(3, -2, by = -.1)
## List of outcome variables
outcome_vars <- c("median_outage_duration_hr", "above_median_cust_affected")

## List of covariates
continuous_vars <-
  names(acs_outages_train %>% select(-c(all_of(outcome_vars), GEOID)))

# Functions
eval_results <- function(model, true, predicted) {
  model <- enquo(model)
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/length(true))

  # Model performance metrics
  data.frame(
    model = as_label(model),
    RMSE = RMSE,
    Rsquare = R_square
  )
}
```

```{r, echo=FALSE}
# Regression df
reg_train <-
  acs_outages_train %>%
  select(-c(GEOID, above_median_cust_affected, prop_white, prop_eli, prop_college, prop_owner, rental_vacancy_rate, prop_rural)) %>%
  drop_na()

reg_test <- 
  acs_outages_test %>% 
  select(-c(GEOID, above_median_cust_affected, prop_white, prop_eli, prop_college, prop_owner, rental_vacancy_rate, prop_rural)) %>%
  drop_na()

# Classification df
# Set up data for classification (and drop NAs)
acs_outages_class_train <- 
  acs_outages_train %>% 
  select(
    -c(GEOID, median_outage_duration_hr)
  ) %>% 
  drop_na() 

# Set up data for classification (and drop NAs)
acs_outages_class_test <- 
  acs_outages_test %>% 
  select(
    -c(GEOID, median_outage_duration_hr)
  ) %>% 
  drop_na() 

# Transform covariates for linear regression
reg_train_transform <-
  reg_train %>%
   mutate(
    prop_latino = log10(prop_latino + 1),
    prop_less_than_hs = log10(prop_less_than_hs + 1)
  )

reg_test_transform <-
  reg_test %>%
   mutate(
    prop_latino = log10(prop_latino + 1),
    prop_less_than_hs = log10(prop_less_than_hs + 1)
  )


# following code is unncessary (for now)
# # Logistic boot model df
# acs_outages_log_train <-
#   acs_outages_train %>% 
#   select(
#     -c(GEOID, median_outage_duration_hr, prop_white, prop_eli, 
#        prop_college, prop_owner, rental_vacancy_rate, prop_rural)
#   ) %>% 
#   drop_na() 
# 
# acs_outages_log_test <-
#   acs_outages_test %>% 
#   select(
#     -c(GEOID, median_outage_duration_hr, prop_white, prop_eli, 
#        prop_college, prop_owner, rental_vacancy_rate, prop_rural)
#   ) %>% 
#   drop_na() 
```

# Prediction on the test set
## Regression

Our best performing linear regression model transformed two covariates, prop_latino and prop_less_than_hs. We selected this model because it had the lowest cross-validation error (4.065). While still having a low R^2 and relatively high CVerror, this model is able to pick up on small changes in the prop_latino and prop_less_than_hs covariates, as they have heavy-tailed distributions, without overfitting the training data. Therefore, we believed it would be more generalizable to the test data.
```{r, ech0=FALSE}
lr_transform <- lm(median_outage_duration_hr ~ ., data = reg_train_transform)

transform_predict <- predict(lr_transform, reg_test_transform)
RMSE(reg_test_transform$median_outage_duration_hr, transform_predict)
```
The resulting RMSE is 3.40194, which is lower than the CVerror. 

## Classification

Our best performing classification model used principal components analysis (PCA) to reduce variables and to account for the colinearity of our covariates. Following PCA, we then used K-Nearest Neighbors (KNN) to determine if an outage affected more than the average amount of customers. Our focus was on reducing 0-1 loss and increasing sensitivity.

```{r, echo=FALSE}
# conduct PCA (to reduce data, especially collinearity)
# Find variance of each covariate
train_col_var <- 
  acs_outages_class_train %>% 
  select(-above_median_cust_affected) %>% 
  mutate(across(everything(), stats::var)) %>% 
  distinct() %>% 
  pivot_longer(cols = everything(), names_to = "vars", values_to = "variance") %>% 
  # filter out 0 variance covariates
  filter(variance != 0)

# remove zero variance covariates
class_filter <-
  acs_outages_class_train %>% 
  # remove outcome var as well
  select(all_of(train_col_var$vars))

# Run PCA
pca <- prcomp(class_filter, scale = T, center = T)
```

```{r, echo=FALSE}
# PCA results
pca_table_train <- 
  data.frame(
    above_median_cust_affected = acs_outages_class_train[,1] %>% pull(), pca$x
  ) %>% 
  #filter to the first 7 components
  select(above_median_cust_affected:PC7) %>% 
  bind_cols()
```

```{r,echo=FALSE}
# Do 10-fold CV KNN
set.seed(243)
# Set train control
knn_control <- 
  trainControl(
    method = "cv", 
    number = 10
  )

# Evaluate accuracy of KNN classifiers
knn_fit <-
  train(
    as.factor(above_median_cust_affected) ~ .,
    method = "knn",
    # Limit to k = sqrt(n)
    tuneGrid = expand.grid(k = 1:sqrt(nrow(pca_table_train))),
    trControl = knn_control,
    metric = "Accuracy",
    data = pca_table_train
  )
```

```{r, echo=FALSE}
# create confusion matrix
confusion_matrix_knn <- confusionMatrix(knn_fit, positive = 1)
confusion_matrix_knn <- confusion_matrix_knn$table %>% t()
```

```{r, echo=FALSE}
# calculate confusion matrix scores
tn <- confusion_matrix_knn[1,1]
fn <- confusion_matrix_knn[2,1]
fp <- confusion_matrix_knn[1,2]
tp <- confusion_matrix_knn[2,2]

train_01_loss <- (fp + fn) / nrow(acs_outages_class_train)
train_sensitivity <- tp / (fn + tp)
train_specificity <- tn / (tn + fp)
train_precision <- tp / (tp + fp)
train_typeI_error <- fp / (tn + fp)
train_typeII_error <- fn / (fn + tp)
train_false_discovery <- fp / (tp + fp)
```

```{r, echo=FALSE}
# conduct PCA (to reduce data, especially collinearity)
# Find variance of each covariate
test_col_var <- 
  acs_outages_class_test %>% 
  select(-above_median_cust_affected) %>% 
  mutate(across(everything(), stats::var)) %>% 
  distinct() %>% 
  pivot_longer(cols = everything(), names_to = "vars", values_to = "variance") %>% 
  # filter out 0 variance covariates
  filter(variance != 0)

# remove zero variance covariates
class_filter <-
  acs_outages_class_test %>% 
  # remove outcome var as well
  select(all_of(test_col_var$vars))

# Run PCA
pca <- prcomp(class_filter, scale = T, center = T)
```

```{r, echo=FALSE}
# PCA results
pca_table_test <- 
  data.frame(
    above_median_cust_affected = acs_outages_class_test[,1] %>% pull(), pca$x
  ) %>% 
  #filter to the first 7 components
  select(above_median_cust_affected:PC7) %>% 
  bind_cols()
```

```{r,echo=FALSE}
# Do 10-fold CV KNN
set.seed(243)
# Set train control
knn_control <- 
  trainControl(
    method = "cv", 
    number = 10
  )

# Evaluate accuracy of KNN classifiers
knn_fit <-
  train(
    as.factor(above_median_cust_affected) ~ .,
    method = "knn",
    # Limit to k = sqrt(n)
    tuneGrid = expand.grid(k = 1:sqrt(nrow(pca_table_test))),
    trControl = knn_control,
    metric = "Accuracy",
    data = pca_table_test
  )
```

```{r, echo=FALSE}
# create confusion matrix
confusion_matrix_knn <- confusionMatrix(knn_fit, positive = 1)
confusion_matrix_knn <- confusion_matrix_knn$table %>% t()
```

```{r, echo=FALSE}
# calculate confusion matrix scores
tn <- confusion_matrix_knn[1,1]
fn <- confusion_matrix_knn[2,1]
fp <- confusion_matrix_knn[1,2]
tp <- confusion_matrix_knn[2,2]

test_01_loss <- (fp + fn) / nrow(acs_outages_class_test)
test_sensitivity <- tp / (fn + tp)
test_specificity <- tn / (tn + fp)
test_precision <- tp / (tp + fp)
test_typeI_error <- fp / (tn + fp)
test_typeII_error <- fn / (fn + tp)
test_false_discovery <- fp / (tp + fp)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# confusion matrix comparison
confusion_matrix_comparison <-
  tibble(
    metric =
      c("Mean 0-1 Loss", "Precision", "Sensitivity", "Specificity",
        "Type I Error Rate", "Type II Error Rate", "False Discovery Rate"),
    train = 
      c(train_01_loss, train_precision, train_sensitivity, train_specificity,
        train_typeI_error, train_typeII_error, train_false_discovery),
    test = 
      c(test_01_loss, test_precision, test_sensitivity, test_specificity,
        test_typeI_error, test_typeII_error, test_false_discovery)
  ) %>% 
  kable(caption = "Above Median Customer Affected Confusion Matrix Metrics") %>% 
  kableExtra::kable_classic() %>% 
  #format for markdown
  kable_styling(latex_options="scale_down")
confusion_matrix_comparison
```

On the test set, it is apparent that mean 0-1 loss increased significantly and sensitivity dropped. Other metrics improved, like precision, while others worsened, like the Type II Error Rate. Given our focus on 0-1 loss and sensitivity, this model had an underwhelming performance. It would have been preferable to maintain a higher sensitivity rate with a lower 0-1 loss, but it is not clear how possible this is due to the low explanatory nature of the covariates.

# Inference
## Parts a) and b) below
```{r}
# variable selection (non-PCA)
# start with basic logistic model (train)
logit_mod_train <-
  glm(
    above_median_cust_affected ~ .,
    data = acs_outages_class_train, 
    family = binomial
  ) %>% 
  # stepwise selection by AIC
  MASS::stepAIC(trace = FALSE, direction = "both")
summary(logit_mod_train)

# extract just the selected variables to run through the bootstrap
acs_outage_var_select <-
  acs_outages_class_train %>% 
  select(above_median_cust_affected, any_of(names(logit_mod_train$coefficients)))

```

* For your chosen model, look at which coefficients are significant in the regression output,
  according to R. In words, describe what statistical significance means for these coefficients.
  Do you believe the results? Why or why not?

For this task, we decided to use a logistic model for classification. Due to the high number of covariates, we used stepwise regression (both directions) to select for variables. The stepwise selection highlighted proportions of white, Black/African American, American Indian and Alaska Native, and multi-racial people, as well as proportion owners, proportion of rural land, and the number of outages by square kilometer. By definition, each chosen covariate in the final model is significant at some level. In other words, these variables are only significant in so far are they are significant compared to the excluded covariates. Therefore, we do not put a lot of stock in the results - as it's quite likely that there are other covariates with much stronger explanatory power that were not included or available in original dataset. Additionally, stepwise selection does not consider all possible models and may be missing important context. We ultimately chose to use stepwise selection due to computational limitations, the low explanatory power of the data itself, and the presence of colinearity.

```{r}
# variable selection (non-PCA)
# start with basic logistic model (test)
logit_mod_test <-
  glm(
    above_median_cust_affected ~ .,
    data = acs_outages_class_test, 
    family = binomial
  ) %>% 
  # stepwise selection by AIC
  MASS::stepAIC(trace = FALSE, direction = "both")
summary(logit_mod_test)
```


* Now fit your chosen model on the test data, and look at whether the same coefficients are
  significant. Did anything change in doing so? If so, explain any differences that you found,
  and reflect on why they might be there.
  
Running the logistic model and stepwise selection on the test data produced different results - proportions of white, Black/African American, and multi-racial people, as well as number of outages by square kilometer remain, but the test data suggests using proportion of Asian people, proportion of college-educated people, and proportion of high school-educated people. These differences are most likely due to the smaller sample size of the test set and the presence of bias. It is likely that in the census tracts available in the test set, it just so happens that education has a more significant relationship with outage size than in the population data.

## Part c)
```{r}
# bootstrap CI for each reg coefficient
# create function to return coefficients
boot_coefficients <- function(data, indices) {
  data <- data[indices,]
  
  logit_mod <-
    glm(
      above_median_cust_affected ~ .,
      # use reduced variables dataset
      data = data, 
      family = binomial
    )
  coefficients(logit_mod)
}

# Need to adjust R value (number of replicates, not sure what to use)
logit_boot <- boot(acs_outage_var_select, boot_coefficients, R = 2000)

# Loop through the results and create table of coefficient CIs
coef_ci_table <- c()
for (i in 1:ncol(acs_outage_var_select)) {
  # run the boot CI on each coefficient
  result <- boot.ci(logit_boot, index = i, type = "norm")
  coef_ci_table <- 
    result$normal %>% 
    as.data.frame() %>% 
    rename(lower_ci = V2, upper_ci = V3) %>% 
    rownames_to_column(var = "coefficient") %>% 
    bind_rows(coef_ci_table)
}

# merge CI table with original coefficient values
coef_comparison_table <-
  coefficients(logit_mod_train) %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "coefficient") %>% 
  rename(value = ".") %>% 
  left_join(coef_ci_table, by = "coefficient")
coef_comparison_table %>% 
  kable(caption = "Coefficient Confidence Intervals") %>% 
  kableExtra::kable_classic() %>% 
  #format for markdown
  kable_styling(latex_options="scale_down")
```

No coefficients lie outside the range of the 95% confidence interval after running the reduced logistic model through the bootstrap. Some of the intervals of relatively large, however, which is of concern (i.e, for proportion multi-racial or proportion American Indian and Alaska Native). Additionally, the fact that proportion of owners has a confidence interval that changes signs is worth noting.

# Inference Discussion
â€¢ If your chosen model does not include all covariates you had available, compare the significant coefficients in your chosen model to the significant coefficients in a model that includes
all covariates. Did the significant coefficients change? If so, explain the differences.

```{r}
# stepwise logistic model (train)
summary(logit_mod_train)
# baseline logistic model (train)
# start with basic logistic model (train)
logit_base_train <-
  glm(
    above_median_cust_affected ~ .,
    data = acs_outages_class_train, 
    family = binomial
  )
summary(logit_base_train)
```
There is some change in significance of coefficients between the chosen model (which does not have all variables) and the full logistic model. For instance, the proportion of owners is not significant at all in the full model full. Additionally, the proportion of Asian people is highly significant in the full model, but is not present at all in the reduce model. Finally, the degree of significance between covariates in both models differs.

* Do you believe collinearity is impacting your results? Be specific.

Yes, collinearity is almost certainly impacting the results. Even in the reduced model, there are multiple covariates that relate to proportions by race - which are certainly collinear variables. The most important impact of this is likely born out in the stepwise selection - the algorithm is choosing artificially inflated covariates to be part of the reduced model.

* Comment on post-selection inference: What aspects of your model-building and inference
process that might bias your determination of which coefficients are significant?

Using the stepwise approach could have biased the initial model in its determination of which covariates were most influential. Again, the issue of collinearity plagues our dataset, and the fact that so many of the final variables in our reduced model with racial demographic proportions highlights this fact. This reduction could have excluded important explanatory variables.

Not really sure what else to add here.

We should consider external validation with new data and specifically integrating data from other California electric utilites and census tracts to strengthen the case for a particular relationship. 



Are confidence intervals wider or narrower on test set? THey should be wider because we are using less data. @Lilla - we didn't produce confidence intervals on the test set.


# Discussion

a) How would you expect your models to be used practically? Do you think they would primarily
be used for prediction, for inference, or for both? What decisions do you think your
models would guide, and what pitfalls do you see in using your models to make these decisions?

While PG&E and other electric utilities monitor their outages and service quality through numerous quantitative metrics, including spatial distribution, there is little emphasis on the communities that are affected by these disruptions in service. Of course, power outages impact residential, commerical, and industrial customers in different ways. However, we must also consider the equity implications of who is being impacted by irregular electricity service. Our models aimed to bring disparities in service quality to light by looking at the relationship between where outages are located and who they impact. Further, they try to help us understand if the demographic make up of a census tract makes a certain area more vulnerable to worse service quality or higher outage durations. Practically, this concept could be used in two spheres. The first is disaster relief and preparation. Suppose our model showed that certain covariates 

PG&E and other electric utilites making decisions about where and for how long to cut power

b) How well would your models hold up over time (i.e., how often do you think they should be
refitted)? Why?

Our models were fitted on power outage data from the last several months, but PG&E and other utilites are constantly collecting new data on their service regularity and variability. Patterns in the frequency and duration of power outages will likely change over time due to the prevalence of wildfires, improvements in transmission and distribution infrastructure, and increased ability to predict wildfire behavior and movement. While the United States Census Bureau performs the census every ten years, the American Community Survey releases new data yearly. Our demographic data is thus relatively up to date and provides a current snapshot of census tract demographics. Therefore, our model will need to be refitted regularly with the newest data on both outages and demographic data. 

** how to integrate changes in land use, satellite data 

c) Are there choices you made in your data analysis, that you would want to make sure any
one (e.g., a manager, a client, etc.) that uses your models is aware of? Examples here might
include approaches to data cleaning; data transformations that you chose; vulnerability to
overfitting, multiple hypothesis testing, or post-selection inference; etc.

- transformed variables with heavy tailed distributions. Because we used proportions for all our variables, had to remove one of each of them to avoid collinearity. 

d) If you could, how would you change the data collection process? In particular, are there
reasonable covariates you would like to collect, that were not present in the data?

PG&E only provides a single coordinate for each outage. Therefore there is not reliable information concerning the extent of the entire affected outage area. We georeferenced the outage coordinate to determine which census tract it corresponded to, though this only allowed us to infer what communities a specific power outage affected.

PG&E does not provide a data dictionary, so we must infer what the different variables mean based on their names. 

Because we only used PG&E outage data, our model was design to elucidate utility service quality in primarily Northern California census tracts. Our entire dataset was only 2722 rows, which is relatively small and may have impacted our ability to find meaningful insights. There are other major electric utilites in California, such as Southern California Edison and San Diego Gas and Electric, that also serve geographies severely impacted by wildfires and with socioeconomic disparities. By building a dataset that included outage data on all 8,057 California census tracts, we would have a more complete picture of electric utility service in California. This would also allow us to understand differences in service between utilities.

e) If you were to attack the same dataset again, what would you do differently?

When embarking on this topic, we believed that demographic characteristics of census tracts would have some predictive power to understand why PG&E's power outages were distributed the way they are. Historically, the quality of public services differ vastly based on communities' socioeconomic status, race, and wealth. After completing our analysis, there are likely many other factors at play in determining PG&E's ability to return power to its customers. Environmental and geographic factors, as well as PG&E's own behind-the-scenes decision making about where to cut power, will surely influence the prevalence and duration of power outages. Environmental and geographic factors that could have benefitted our model include the size, proximity, and presence of wildfire; quality of transmission and distribution infrastructure (including powerlines and voltage boxes); more detailed land cover data (vegetation, temperative, albedo, forested area); and building infrastructure data (industrial and commercial offtakers use more electricity than residential customers). Perhaps it would have been wiser to try to model how PG&E itself makes choices about where and how long to cut power for. 

On one hand, it is a good thing that PG&E's service does not seemingly discriminate against certain groups based on the proportion of that demographic indicator present in a census tract. 

External validation using subsequent studies.




