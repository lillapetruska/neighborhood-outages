% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode=true}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=blue,
}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{sectsty} \allsectionsfont{\centering}
\usepackage{indentfirst}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\noindent Matt Alvarez-Nissen and Lilla Petruska

\noindent MS\&E 226

\noindent 10/27/2020

\allsectionsfont{\centering}

\hypertarget{mse-226-project-part-1---neighborhood-outages}{%
\subsection{MS\&E 226 Project Part 1 - Neighborhood
Outages}\label{mse-226-project-part-1---neighborhood-outages}}

Pacific Gas \& Electric (PG\&E) has made headlines over the last few
years, largely due to blame over faulty equipment deemed responsible for
sparking California's worst wildfires in that time. As a result, the
utility company has imposed ``public safety power shutoffs'' in the wake
of dangerous fire conditions to reduce the possibility of their
equipment contributing to any new wildfires. It is also common for
communities to experience power shutoffs during distant wildfires or
shortly after they start as they might prevent their spread. For our
project, we investigate these PG\&E outages and what factors influence
their scope (temporally and spatially). Specifically, are there factors
in communities that impact PG\&E's service to its customers and response
to such power outages?

\hypertarget{investigating-the-data}{%
\subsubsection{Investigating the data}\label{investigating-the-data}}

By combining data on PG\&E outages with demographic information, we hope
to answer some questions about which characteristics within Census
tracts are associated with the duration and extent of power outages. Are
there certain racial, socioeconomic, or educational attainment
indicators within communities that influence PG\&E's response time to
outages? Does PG\&E's performance show neglect for its customers in
certain Census tracts? Is median duration per tract higher if there is a
higher proportion of non-white residents? Given the contentious nature
of PG\&E's performance over the last few years, it is especially
relevant now to understand how their electricity system infrastructure
impacts Californians and it is imperative that we focus this
investigation through the lens of equity. Predicting characteristics
about PG\&E's outage will be increasingly important in the future as
wildfires become more frequent and severe. Understanding what impacts
outage duration and extent can give us insights into which populations
might be vulnerable to being left without power for longer periods of
time and thus inform decisions about where energy infrastructure may
need to be updated and strengthened in preparation for times of crises.

Our dataset combines data from three sources: the United States Census
Bureau's 2018 American Community Survey, the 2010 Decennial Census
urban-rural classification of Census tracts, and PG\&E electricity
outage data for the last year (found
\href{https://simonwillison.net/2019/Oct/10/pge-outages/}{here}). The
first provides demographic data at the Census tract level in California,
including information on race, population density, income level,
educational attainment, tenure (homeowner vs renter), and home vacancy
rates. The second gives information about urban versus rural land use in
tracts. The third was scraped from PG\&E directly by Simon WIllison and
contains information on outage duration, location, and customers
affected. By georeferencing each outage, we were able to calculate a set
of summary statistics of PG\&E outages for each Census tract that is
within PG\&E's service area. Our dataset for analysis is the result of
joining these three separate datasets, giving us demographic, land use,
and outage data for 2722 Census tracts in California.

We have a few concerns about the quality of our data, including that the
location data PG\&E provides is only a single coordinate for each
outage, and thus there is not reliable information concerning the extent
of the entire affected outage area. Additionally, PG\&E does not provide
a data dictionary, so we must infer what the different variables mean
based on their names. Another issue in the structure of the data is that
While PG\&E serves a majority of Northern California, the geography has
an incredibly diverse demographic and environmental makeup. Our random
train-test split hopefully accounted for variation across the Census
tracts, but there is a possibility that the tracts within each of these
groups share similar characteristics. Finally, our data had relatively
few observations, as we only consider the Census tracts with outages. We
deemed this, as opposed to including all Census tracts in California,
the more appropriate subset to analyze. Given that PG\&E's service area
is restricted, we felt including all Census tracts would unfairly weight
the \texttt{0} observations for our outcome variables. We realize,
however, that this must introduce some selection bias. If allowed more
time, we would also deploy our models on the entirety of PG\&E's service
area (which was not immediately available).

Data processing was a three step process, requiring Census data
cleaning, PG\&E data cleaning, and merging. 2018 ACS measurements for
race, population density, income (as percentage of Area Median Income)
level, educational attainment, tenure (homeowner vs renter), and home
vacancy rates were calculated as proportions. Additionally, we used 2010
Decennial Census (the most recent data available) to determine the
proportion of the population classified as urban or rural. We removed
one covariate from each of category to reduce issues with collinearity
in the regression model. The \texttt{tigris} and \texttt{tidycensus}
packages were used to gather and clean all Census data. Outage data was
taken from Simon Willison's PG\&E
\href{https://simonwillison.net/2019/Oct/10/pge-outages/}{scraper}, and
was filtered to only include outages that were over. There is about a
year of data, from October 2019 - October 2020. Mean customers affected
was derived by taking the average of \texttt{min\_estCustAffected} and
\texttt{max\_estCustAffected}, and \texttt{possible\_duration\_hours}
was used as a measure of outage duration in hours. This data was
georeferenced to Census tracts using latitude and longitude, with help
from the \texttt{tigris} and \texttt{sf} packages. ACS and outage data
were then merged by grouping outage data by tract, after determining
median outage duration, median customers affected, and total number of
outages by tract. A binary flag was generated to note if a tract had
above median customers affected by outages. We also generated a density
statistic for outages per square km. Full data cleaning scripts are
available in \protect\hyperlink{appendixC}{Appendix C}.

Our continuous response variable is the median outage duration (hours)
in each Census tract. We chose this variable because it may serve as a
proxy for PG\&E service quality (how fast PG\&E can bring power back on
for communities). Our binary response variable determines if the number
of customers affected by outages within a Census tract is above the
median. We chose this because it may help us understand what kind of
neighborhoods are more heavily impacted by these power outages.

\begin{table}

\caption{\label{tab:unnamed-chunk-3}Very High Correlations Amongst Covariates}
\centering
\resizebox{\linewidth}{!}{
\fontsize{2}{4}\selectfont
\begin{tabular}[t]{l|l|r}
\hline
covariate\_1 & covariate\_2 & correlation\\
\hline
prop\_less\_than\_hs & prop\_college & -0.7904132\\
\hline
prop\_high\_school & prop\_college & -0.7850177\\
\hline
prop\_college & prop\_latino & -0.7607331\\
\hline
prop\_vli & prop\_hi & -0.7408347\\
\hline
prop\_li & prop\_hi & -0.7171673\\
\hline
prop\_mi & prop\_hi & -0.6743541\\
\hline
prop\_less\_than\_hs & prop\_white & -0.6725149\\
\hline
prop\_latino & prop\_white & -0.6717423\\
\hline
prop\_renter & prop\_hi & -0.6274611\\
\hline
n\_outages\_sq\_km & pop\_density\_sq\_km & 0.6993085\\
\hline
\end{tabular}}
\end{table}

After exploring our data, it became clear to us that correlations
between covariates would be a major issue to consider, especially given
the high correlation between variables like education and race. For the
regression model, we thus decided to include interaction terms in our
data to capture this relationship. We also included log transformations
on variables following a chi-squared distribution, including proportion
Latino and proportion with less than a high school education. There were
also a handful (about 15) Census tracts that contained \texttt{NA}
values, and most of those with \texttt{NA}s did not have other useful
information so we decided to drop them. Finally, our EDA let us see some
interesting patterns concerning race, education, and tenure especially.
As median outage duration increased, the proportion of white residents,
college-educated residents, and owners decreased. These patterns
demonstrated to us that our data had at least some worthwhile predictive
power. These plots are included in
\protect\hyperlink{appendixD}{Appendix D}.

\hypertarget{predictions}{%
\subsubsection{Predictions}\label{predictions}}

Here we will discuss the general modeling process and decisions made in
creating both regression and classification models. The full process and
code can be found in \protect\hyperlink{appendixB}{Appendix B}.

Our regression model was built to predict our continuous outcome
variable, median outage duration (hours), in each tract. For a baseline
model, we used a basic OLS linear regression.

\begin{table}

\caption{\label{tab:unnamed-chunk-4}Prediction Error, R^2, and CVerror for Regression Models}
\centering
\resizebox{\linewidth}{!}{
\fontsize{2}{4}\selectfont
\begin{tabular}[t]{l|r|r|r}
\hline
model & RMSE & Rsquare & CVerror\\
\hline
transform & 4.030581 & 0.0388899 & 4.069945\\
\hline
all interactions & 3.856560 & 0.1200903 & 4.216894\\
\hline
transform + interactions & 3.850536 & 0.1228373 & 4.279210\\
\hline
selected transform + interactions & 4.012492 & 0.0474975 & 4.077540\\
\hline
ridge & 4.037205 & 0.0357280 & 4.110233\\
\hline
lasso & 4.031624 & 0.0383926 & 4.111318\\
\hline
\end{tabular}}
\end{table}

In classification, our major goal was to identify as many true positives
as possible, because we wanted a model optimized for proactive
decision-making. In other words, we were fine with relatively higher
false positives, because the model is designed to give warning to
particular neighborhood types if they are at higher risk of having high
numbers of customers affected by outages. As such our major goals were
to reduce mean 0-1 loss and increase sensitivity.

For a baseline, we used a logistic regression of the model including all
covariates. This performed fairly well, with a mean 0-1 loss of about
.193 and sensitivity of .067. However, given the issues of collinearity
pointed out before, we wanted to see if we could develop a better model
that reduced these issues. Thus, we used Principal Components Analysis
(PCA) to reduce the covariates, and chose to include the first 7
components (which explained about 70\% of the variance). Next, we used
K-nearest neighbors (KNN) on those first 7 components, and did a 10-fold
cross-validation to derive the optimal K (which was 37). Using confusion
matrices, we then produced a comparative table between the two models
(shown in Table 2). Mean 0-1 loss greatly improved, now about .009 while
sensitivity increased to about .095. Obviously other metrics took a hit,
including AUC, specificity, and precision. For the purposes of our model
though, this was deemed acceptable.

Overall, there is reason to believe that our model is overly optimistic
and biased. While CV was employed to reduce this possibility, the
relatively low explanatory power of our covariates make this more
likely. Additionally, the number of components chosen was relatively
arbitrary. 7 components occurred after the break in the scree plots, but
was chosen because it explained an acceptable amount the variance. KNN
might have also been a suboptimal classification approach. However,
given the constraints of our data, we were content with the KNN model.
We felt it made use of the highest explanatory power of the data without
overfitting to noise excessively. Hopefully this remains true when we
deploy it on the test set.

\begin{table}

\caption{\label{tab:unnamed-chunk-13}Above Median Customer Affected Confusion Matrix Metrics}
\centering
\resizebox{\linewidth}{!}{
\fontsize{2}{4}\selectfont
\begin{tabular}[t]{l|r|r}
\hline
metric & base & final\\
\hline
Mean 0-1 Loss & 0.1927878 & 0.0088275\\
\hline
Precision & 0.6444444 & 0.6307692\\
\hline
Sensitivity & 0.0674419 & 0.0953488\\
\hline
Specificity & 0.9907675 & 0.9861512\\
\hline
Type I Error Rate & 0.0092325 & 0.0138488\\
\hline
Type II Error Rate & 0.9325581 & 0.9046512\\
\hline
False Discovery Rate & 0.3555556 & 0.3692308\\
\hline
\end{tabular}}
\end{table}

\newpage

\hypertarget{appendixA}{%
\subsection{Appendix A - Report Setup}\label{appendixA}}

This appendix provides the code used to setup up this report in
RMarkdown.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Please exclude this page from page count}
\CommentTok{# Libraries}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(Hmisc)}
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(foreach)}
\KeywordTok{library}\NormalTok{(skimr)}
\KeywordTok{library}\NormalTok{(knitr)}
\KeywordTok{library}\NormalTok{(kableExtra)}
\KeywordTok{library}\NormalTok{(GGally)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(pROC)}
\KeywordTok{library}\NormalTok{(here)}
\KeywordTok{library}\NormalTok{(glmnet)}
\KeywordTok{library}\NormalTok{(corrr)}
\KeywordTok{library}\NormalTok{(elasticnet)}
\KeywordTok{library}\NormalTok{(cvTools)}

\CommentTok{# Load Data}
\NormalTok{acs_outages <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(here}\OperatorTok{::}\KeywordTok{here}\NormalTok{(), }\StringTok{"/cleaned_data/acs_outages_train.csv"}\NormalTok{))}

\CommentTok{# Parameters}
\NormalTok{lambdas <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{-2}\NormalTok{, }\DataTypeTok{by =} \FloatTok{-.1}\NormalTok{)}
\CommentTok{## List of outcome variables}
\NormalTok{outcome_vars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"median_outage_duration_hr"}\NormalTok{, }\StringTok{"above_median_cust_affected"}\NormalTok{)}

\CommentTok{## List of covariates}
\NormalTok{continuous_vars <-}
\StringTok{  }\KeywordTok{names}\NormalTok{(acs_outages }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), GEOID)))}

\CommentTok{# Functions}
\NormalTok{eval_results <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(model, true, predicted) \{}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{enquo}\NormalTok{(model)}
\NormalTok{  SSE <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((predicted }\OperatorTok{-}\StringTok{ }\NormalTok{true)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  SST <-}\StringTok{ }\KeywordTok{sum}\NormalTok{((true }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(true))}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{  R_square <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{SSE }\OperatorTok{/}\StringTok{ }\NormalTok{SST}
\NormalTok{  RMSE =}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(SSE}\OperatorTok{/}\KeywordTok{length}\NormalTok{(true))}

  \CommentTok{# Model performance metrics}
  \KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{model =} \KeywordTok{as_label}\NormalTok{(model),}
    \DataTypeTok{RMSE =}\NormalTok{ RMSE,}
    \DataTypeTok{Rsquare =}\NormalTok{ R_square}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Regression df}
\NormalTok{reg_train <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(GEOID, above_median_cust_affected, prop_white, prop_eli, prop_college, prop_owner, rental_vacancy_rate, prop_rural)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{drop_na}\NormalTok{()}

\CommentTok{# Classification df}
\CommentTok{# Set up data for classification (and drop NAs)}
\NormalTok{acs_outages_class <-}\StringTok{ }
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}
    \OperatorTok{-}\KeywordTok{c}\NormalTok{(GEOID, median_outage_duration_hr)}
\NormalTok{  ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{drop_na}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{appendixB}{%
\subsection{Appendix B - Prediction Models}\label{appendixB}}

This appendix provides the code used to develop both the regression and
classification models.

\hypertarget{regression}{%
\subsubsection{Regression}\label{regression}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Baseline model}
\NormalTok{lr_baseline <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(median_outage_duration_hr }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ reg_train)}

\NormalTok{baseline_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    lr_baseline,}
\NormalTok{    reg_train}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    lr_baseline}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{  )}

\NormalTok{baseline.cv <-}
\StringTok{    }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    lr_baseline,}
    \DataTypeTok{data =}\NormalTok{ reg_train,}
    \DataTypeTok{y =}\NormalTok{ reg_train}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Apply transformations to heavy tailed covariates}
\NormalTok{reg_train_transform <-}
\StringTok{  }\NormalTok{reg_train }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{prop_latino =} \KeywordTok{log10}\NormalTok{(prop_latino }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{),}
    \DataTypeTok{prop_less_than_hs =} \KeywordTok{log10}\NormalTok{(prop_less_than_hs }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Transformation model}
\NormalTok{lr_transform <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(median_outage_duration_hr }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ reg_train_transform)}

\NormalTok{transform_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    lr_transform,}
\NormalTok{    reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    lr_transform}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{  )}

\NormalTok{transform.cv <-}
\StringTok{    }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    lr_transform,}
    \DataTypeTok{data =}\NormalTok{ reg_train_transform,}
    \DataTypeTok{y =}\NormalTok{ reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# All interactions model}
\NormalTok{lr_interact <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(median_outage_duration_hr }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\NormalTok{.}\OperatorTok{:}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ reg_train)}

\NormalTok{interact_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    lr_interact,}
\NormalTok{    reg_train}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    lr_interact}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{  )}

\NormalTok{interact.cv <-}
\StringTok{    }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    lr_interact,}
    \DataTypeTok{data =}\NormalTok{ reg_train,}
    \DataTypeTok{y =}\NormalTok{ reg_train}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Transformed data with all interactions model}
\NormalTok{lr_transform_interact <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(median_outage_duration_hr }\OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{+}\StringTok{ }\NormalTok{.}\OperatorTok{:}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ reg_train_transform)}

\NormalTok{transform_interact_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    lr_transform_interact,}
\NormalTok{    reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    lr_transform_interact}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{  )}

\NormalTok{transform_interact.cv <-}
\StringTok{    }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    lr_transform_interact,}
    \DataTypeTok{data =}\NormalTok{ reg_train_transform,}
    \DataTypeTok{y =}\NormalTok{ reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# More specific selected covariates}
\CommentTok{# find variables most correlated to each other}
\CommentTok{# (select correlations w/ abs > .5)}
\NormalTok{covariate_corr <-}
\StringTok{  }\NormalTok{reg_train }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{where}\NormalTok{(}\OperatorTok{~}\KeywordTok{sd}\NormalTok{(., }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{correlate}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{stretch}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(r) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{abs}\NormalTok{(r) }\OperatorTok{>}\StringTok{ }\FloatTok{.5}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{lead_y =} \KeywordTok{lead}\NormalTok{(y)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(x }\OperatorTok{!=}\StringTok{ }\NormalTok{lead_y) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Correlation method: 'pearson'
## Missing treated using: 'pairwise.complete.obs'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Reformat in formula form}
\NormalTok{interaction_vars <-}
\StringTok{  }\NormalTok{covariate_corr }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unite}\NormalTok{(}\DataTypeTok{col =} \StringTok{"interact"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{":"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{paste}\NormalTok{(}\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)}

\CommentTok{# Pulled list of top covariates}
\NormalTok{lr_selected_interact_transform <-}
\StringTok{  }\KeywordTok{lm}\NormalTok{(}
    \KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"median_outage_duration_hr ~ . +"}\NormalTok{, interaction_vars)),}
    \DataTypeTok{data =}\NormalTok{ reg_train_transform}
\NormalTok{  )}

\NormalTok{log_selected_interact_transform_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    lr_selected_interact_transform,}
\NormalTok{    reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    lr_selected_interact_transform}\OperatorTok{$}\NormalTok{fitted.values}
\NormalTok{  )}

\NormalTok{log_selected_interact_transform.cv <-}
\StringTok{    }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    lr_selected_interact_transform,}
    \DataTypeTok{data =}\NormalTok{ reg_train_transform,}
    \DataTypeTok{y =}\NormalTok{ reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\NormalTok{reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr}
\NormalTok{x <-}\StringTok{ }\NormalTok{reg_train_transform }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{median_outage_duration_hr) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{data.matrix}\NormalTok{()}

\CommentTok{# Ridge}
\NormalTok{cv_ridge <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ lambdas, }\DataTypeTok{standardize =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{ridge_model <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ cv_ridge}\OperatorTok{$}\NormalTok{lambda.min)}

\NormalTok{ridge_predict <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridge_model, x)}

\NormalTok{ridge_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    ridge_model,}
\NormalTok{    reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    ridge_predict}
\NormalTok{  )}

\NormalTok{ridge.cv <-}\StringTok{ }
\StringTok{  }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    cv_ridge,}
\NormalTok{    x,}
    \DataTypeTok{y =}\NormalTok{ reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}


\CommentTok{# Lasso}
\NormalTok{cv_lasso <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ lambdas, }\DataTypeTok{standardize =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{lasso_model <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ cv_lasso}\OperatorTok{$}\NormalTok{lambda.min)}

\NormalTok{lasso_predict <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lasso_model, x)}

\NormalTok{lasso_results <-}
\StringTok{  }\KeywordTok{eval_results}\NormalTok{(}
\NormalTok{    lasso_model,}
\NormalTok{    reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
\NormalTok{    lasso_predict}
\NormalTok{  )}

\NormalTok{lasso.cv <-}\StringTok{ }
\StringTok{  }\KeywordTok{cvFit}\NormalTok{(}
\NormalTok{    cv_lasso,}
\NormalTok{    x,}
    \DataTypeTok{y =}\NormalTok{ reg_train_transform}\OperatorTok{$}\NormalTok{median_outage_duration_hr,}
    \DataTypeTok{K =} \DecValTok{10}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}
\StringTok{  }\KeywordTok{tribble}\NormalTok{(}
    \OperatorTok{~}\NormalTok{model, }\OperatorTok{~}\NormalTok{RMSE, }\OperatorTok{~}\NormalTok{Rsquare, }\OperatorTok{~}\NormalTok{CVerror,}
    \StringTok{"baseline"}\NormalTok{, baseline_results}\OperatorTok{$}\NormalTok{RMSE, baseline_results}\OperatorTok{$}\NormalTok{Rsquare, baseline.cv[[}\StringTok{"cv"}\NormalTok{]],}
    \StringTok{"transform"}\NormalTok{, transform_results}\OperatorTok{$}\NormalTok{RMSE, transform_results}\OperatorTok{$}\NormalTok{Rsquare, transform.cv[[}\StringTok{"cv"}\NormalTok{]],}
    \StringTok{"all interactions"}\NormalTok{, interact_results}\OperatorTok{$}\NormalTok{RMSE, interact_results}\OperatorTok{$}\NormalTok{Rsquare, interact.cv[[}\StringTok{"cv"}\NormalTok{]],}
    \StringTok{"transform + interactions"}\NormalTok{, transform_interact_results}\OperatorTok{$}\NormalTok{RMSE, transform_interact_results}\OperatorTok{$}\NormalTok{Rsquare, transform_interact.cv[[}\StringTok{"cv"}\NormalTok{]],}
    \StringTok{"selected transform + interactions"}\NormalTok{, log_selected_interact_transform_results}\OperatorTok{$}\NormalTok{RMSE, log_selected_interact_transform_results}\OperatorTok{$}\NormalTok{Rsquare, log_selected_interact_transform.cv[[}\StringTok{"cv"}\NormalTok{]],}
    \StringTok{"ridge"}\NormalTok{, ridge_results}\OperatorTok{$}\NormalTok{RMSE, ridge_results}\OperatorTok{$}\NormalTok{Rsquare, ridge.cv[[}\StringTok{"cv"}\NormalTok{]],}
    \StringTok{"lasso"}\NormalTok{, lasso_results}\OperatorTok{$}\NormalTok{RMSE, lasso_results}\OperatorTok{$}\NormalTok{Rsquare, lasso.cv[[}\StringTok{"cv"}\NormalTok{]]}
\NormalTok{  )}

\KeywordTok{write_rds}\NormalTok{(df, }\StringTok{"~/GitHub/neighborhood-outages/cleaned_data/reg_stats.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{classification}{%
\subsubsection{Classification}\label{classification}}

\hypertarget{baseline-model-logistic-regression}{%
\paragraph{Baseline model (logistic
regression)}\label{baseline-model-logistic-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Look at results of the model}
\NormalTok{logit_mod <-}
\StringTok{  }\KeywordTok{glm}\NormalTok{(}
\NormalTok{    above_median_cust_affected }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
    \DataTypeTok{data =}\NormalTok{ acs_outages_class, }
    \DataTypeTok{family =}\NormalTok{ binomial}
\NormalTok{  )}
\KeywordTok{summary}\NormalTok{(logit_mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = above_median_cust_affected ~ ., family = binomial, 
##     data = acs_outages_class)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4979  -0.7019  -0.5208  -0.3062   2.6394  
## 
## Coefficients: (5 not defined because of singularities)
##                                                   Estimate Std. Error z value
## (Intercept)                                     -2.553e+00  1.342e+00  -1.903
## prop_white                                       3.038e+00  6.892e-01   4.408
## prop_black_or_african_american                   2.662e+00  1.281e+00   2.079
## prop_american_indian_and_alaska_native           9.435e+00  4.340e+00   2.174
## prop_asian                                       2.537e+00  6.821e-01   3.719
## prop_native_hawaiian_and_other_pacific_islander  1.258e+00  6.707e+00   0.188
## prop_some_other_race                             3.717e+00  9.133e+00   0.407
## prop_multi_racial                               -6.932e+00  2.892e+00  -2.397
## prop_latino                                             NA         NA      NA
## pop_density_sq_km                                3.334e-05  2.255e-05   1.478
## prop_eli                                        -2.342e+00  2.723e+00  -0.860
## prop_hi                                         -7.690e-01  1.289e+00  -0.597
## prop_li                                         -1.453e+00  2.051e+00  -0.708
## prop_mi                                          2.409e-01  2.032e+00   0.119
## prop_vli                                                NA         NA      NA
## prop_college                                     4.837e-01  1.147e+00   0.422
## prop_high_school                                 2.329e-01  1.210e+00   0.193
## prop_less_than_hs                                       NA         NA      NA
## prop_owner                                      -3.270e-01  3.976e-01  -0.822
## prop_renter                                             NA         NA      NA
## rental_vacancy_rate                              1.163e+00  1.093e+00   1.064
## owner_vacancy_rate                               2.839e+00  2.606e+00   1.089
## prop_rural                                       4.940e-01  2.266e-01   2.180
## prop_urban                                              NA         NA      NA
## n_outages_sq_km                                 -3.509e-02  7.773e-03  -4.515
##                                                 Pr(>|z|)    
## (Intercept)                                       0.0571 .  
## prop_white                                      1.04e-05 ***
## prop_black_or_african_american                    0.0377 *  
## prop_american_indian_and_alaska_native            0.0297 *  
## prop_asian                                        0.0002 ***
## prop_native_hawaiian_and_other_pacific_islander   0.8512    
## prop_some_other_race                              0.6840    
## prop_multi_racial                                 0.0165 *  
## prop_latino                                           NA    
## pop_density_sq_km                                 0.1393    
## prop_eli                                          0.3898    
## prop_hi                                           0.5507    
## prop_li                                           0.4788    
## prop_mi                                           0.9056    
## prop_vli                                              NA    
## prop_college                                      0.6732    
## prop_high_school                                  0.8473    
## prop_less_than_hs                                     NA    
## prop_owner                                        0.4108    
## prop_renter                                           NA    
## rental_vacancy_rate                               0.2874    
## owner_vacancy_rate                                0.2759    
## prop_rural                                        0.0292 *  
## prop_urban                                            NA    
## n_outages_sq_km                                 6.33e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2157.5  on 2162  degrees of freedom
## Residual deviance: 1956.2  on 2143  degrees of freedom
## AIC: 1996.2
## 
## Number of Fisher Scoring iterations: 5
\end{verbatim}

AIC is 1995.3.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create confusion matrix}
\NormalTok{pred <-}\StringTok{ }
\StringTok{  }\KeywordTok{if_else}\NormalTok{(logit_mod}\OperatorTok{$}\NormalTok{fitted.values }\OperatorTok{>}\StringTok{ }\FloatTok{0.5}\NormalTok{, }\StringTok{"1"}\NormalTok{, }\StringTok{"0"}\NormalTok{)}

\NormalTok{confusion_matrix <-}
\StringTok{  }\KeywordTok{table}\NormalTok{(acs_outages_class}\OperatorTok{$}\NormalTok{above_median_cust_affected, pred)}
\KeywordTok{rownames}\NormalTok{(confusion_matrix) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obs. 0"}\NormalTok{, }\StringTok{"Obs. 1"}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(confusion_matrix) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Pred. 0"}\NormalTok{, }\StringTok{"Pred. 1"}\NormalTok{)}
\NormalTok{confusion_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         pred
##          Pred. 0 Pred. 1
##   Obs. 0    1717      16
##   Obs. 1     401      29
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate confusion matrix scores}
\NormalTok{tn <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{fn <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{fp <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{tp <-}\StringTok{ }\NormalTok{confusion_matrix[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}

\NormalTok{base_}\DecValTok{01}\NormalTok{_loss <-}\StringTok{ }\NormalTok{(fp }\OperatorTok{+}\StringTok{ }\NormalTok{fn) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(acs_outages_class)}
\NormalTok{base_sensitivity <-}\StringTok{ }\NormalTok{tp }\OperatorTok{/}\StringTok{ }\NormalTok{(fn }\OperatorTok{+}\StringTok{ }\NormalTok{tp)}
\NormalTok{base_specificity <-}\StringTok{ }\NormalTok{tn }\OperatorTok{/}\StringTok{ }\NormalTok{(tn }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{base_precision <-}\StringTok{ }\NormalTok{tp }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{base_typeI_error <-}\StringTok{ }\NormalTok{fp }\OperatorTok{/}\StringTok{ }\NormalTok{(tn }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{base_typeII_error <-}\StringTok{ }\NormalTok{fn }\OperatorTok{/}\StringTok{ }\NormalTok{(fn }\OperatorTok{+}\StringTok{ }\NormalTok{tp)}
\NormalTok{base_false_discovery <-}\StringTok{ }\NormalTok{fp }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot ROC Curve}
\KeywordTok{roc}\NormalTok{(}
  \DataTypeTok{response =}\NormalTok{ acs_outages_class}\OperatorTok{$}\NormalTok{above_median_cust_affected,}
  \DataTypeTok{predictor =}\NormalTok{ logit_mod}\OperatorTok{$}\NormalTok{fitted.values, }
  \DataTypeTok{data =}\NormalTok{ acs_outages_class, }
  \DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{main =} \StringTok{"Baseline ROC"}\NormalTok{,}
  \DataTypeTok{col =} \StringTok{"blue"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-27-1.pdf}

\begin{verbatim}
## 
## Call:
## roc.default(response = acs_outages_class$above_median_cust_affected,     predictor = logit_mod$fitted.values, plot = TRUE, data = acs_outages_class,     main = "Baseline ROC", col = "blue")
## 
## Data: logit_mod$fitted.values in 1733 controls (acs_outages_class$above_median_cust_affected 0) < 430 cases (acs_outages_class$above_median_cust_affected 1).
## Area under the curve: 0.7106
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# determine AUC}
\KeywordTok{auc}\NormalTok{(}
  \DataTypeTok{response =}\NormalTok{ acs_outages_class}\OperatorTok{$}\NormalTok{above_median_cust_affected,}
  \DataTypeTok{predictor =}\NormalTok{ logit_mod}\OperatorTok{$}\NormalTok{fitted.values,  }
  \DataTypeTok{data =}\NormalTok{ acs_outages_class}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Area under the curve: 0.7106
\end{verbatim}

Major goal is optimize mean 0-1 loss and increase sensitivity. Logistic
model is a good baseline, but I want to see if I can improve those
metrics. Will use PCA to reduce covariates and eliminate some of the
collinearity noticed in EDA. Then I'll take the factor loadings to
conduct KNN classification. Will do a 10-fold cross-validation to ensure
robustness of approach.

\hypertarget{final-model-pca-based-k-nearest-neighbors-algorithm}{%
\paragraph{Final model (PCA-based K-nearest neighbors
algorithm)}\label{final-model-pca-based-k-nearest-neighbors-algorithm}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# conduct PCA (to reduce data, especially collinearity)}
\CommentTok{# Find variance of each covariate}
\NormalTok{train_col_var <-}\StringTok{ }
\StringTok{  }\NormalTok{acs_outages_class }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{above_median_cust_affected) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\KeywordTok{across}\NormalTok{(}\KeywordTok{everything}\NormalTok{(), stats}\OperatorTok{::}\NormalTok{var)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{distinct}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}\DataTypeTok{cols =} \KeywordTok{everything}\NormalTok{(), }\DataTypeTok{names_to =} \StringTok{"vars"}\NormalTok{, }\DataTypeTok{values_to =} \StringTok{"variance"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# filter out 0 variance covariates}
\StringTok{  }\KeywordTok{filter}\NormalTok{(variance }\OperatorTok{!=}\StringTok{ }\DecValTok{0}\NormalTok{)}

\CommentTok{# remove zero variance covariates}
\NormalTok{class_filter <-}
\StringTok{  }\NormalTok{acs_outages_class }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{# remove outcome var as well}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(train_col_var}\OperatorTok{$}\NormalTok{vars))}

\CommentTok{# Run PCA}
\NormalTok{pca <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(class_filter, }\DataTypeTok{scale =}\NormalTok{ T, }\DataTypeTok{center =}\NormalTok{ T)}
\KeywordTok{summary}\NormalTok{(pca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Importance of components:
##                           PC1    PC2     PC3     PC4     PC5     PC6     PC7
## Standard deviation     2.4324 2.0254 1.48899 1.17335 1.10399 1.02003 0.99866
## Proportion of Variance 0.2465 0.1709 0.09238 0.05736 0.05078 0.04335 0.04155
## Cumulative Proportion  0.2465 0.4174 0.50983 0.56720 0.61798 0.66133 0.70289
##                            PC8     PC9    PC10    PC11    PC12    PC13    PC14
## Standard deviation     0.94780 0.93613 0.89918 0.89066 0.85196 0.80226 0.74272
## Proportion of Variance 0.03743 0.03651 0.03369 0.03305 0.03024 0.02682 0.02298
## Cumulative Proportion  0.74032 0.77683 0.81052 0.84357 0.87382 0.90064 0.92362
##                           PC15    PC16    PC17    PC18    PC19      PC20
## Standard deviation     0.71380 0.71199 0.68150 0.48556 0.34127 1.344e-15
## Proportion of Variance 0.02123 0.02112 0.01935 0.00982 0.00485 0.000e+00
## Cumulative Proportion  0.94485 0.96597 0.98532 0.99515 1.00000 1.000e+00
##                             PC21      PC22      PC23      PC24
## Standard deviation     7.241e-16 6.261e-16 5.094e-16 3.306e-16
## Proportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00
## Cumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute standard deviation of each principal component}
\NormalTok{std_dev <-}\StringTok{ }\NormalTok{pca}\OperatorTok{$}\NormalTok{sdev}
\CommentTok{# compute variance}
\NormalTok{pr_var <-}\StringTok{ }\NormalTok{std_dev}\OperatorTok{^}\DecValTok{2}
\CommentTok{# proportion of variance explained}
\NormalTok{prop_var <-}\StringTok{ }\NormalTok{pr_var}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(pr_var)}

\CommentTok{# generate scree plot}
\KeywordTok{plot}\NormalTok{(}
\NormalTok{  prop_var, }
  \DataTypeTok{xlab =} \StringTok{"Principal Component"}\NormalTok{,}
  \DataTypeTok{ylab =} \StringTok{"Proportion of Variance Explained"}\NormalTok{,}
  \DataTypeTok{type =} \StringTok{"b"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-30-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot Eigenvalue variance from PCA}
\CommentTok{#factoextra::fviz_eig(pca)}
\end{Highlighting}
\end{Shaded}

Around 7-8 components explains about 75\% of variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# PCA results}
\NormalTok{pca_table <-}\StringTok{ }
\StringTok{  }\KeywordTok{data.frame}\NormalTok{(}
    \DataTypeTok{above_median_cust_affected =}\NormalTok{ acs_outages_class[,}\DecValTok{1}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(), pca}\OperatorTok{$}\NormalTok{x}
\NormalTok{  ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{#filter to the first 7 components}
\StringTok{  }\KeywordTok{select}\NormalTok{(above_median_cust_affected}\OperatorTok{:}\NormalTok{PC7) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{bind_cols}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{243}\NormalTok{)}
\CommentTok{# Do 10-fold CV KNN}
\CommentTok{# Set train control}
\NormalTok{knn_control <-}\StringTok{ }
\StringTok{  }\KeywordTok{trainControl}\NormalTok{(}
    \DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }
    \DataTypeTok{number =} \DecValTok{10}
\NormalTok{  )}

\CommentTok{# Evaluate accuracy of KNN classifiers}
\NormalTok{knn_fit <-}
\StringTok{  }\KeywordTok{train}\NormalTok{(}
    \KeywordTok{as.factor}\NormalTok{(above_median_cust_affected) }\OperatorTok{~}\StringTok{ }\NormalTok{.,}
    \DataTypeTok{method =} \StringTok{"knn"}\NormalTok{,}
    \CommentTok{# Limit to k = sqrt(n)}
    \DataTypeTok{tuneGrid =} \KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{k =} \DecValTok{1}\OperatorTok{:}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(pca_table))),}
    \DataTypeTok{trControl =}\NormalTok{ knn_control,}
    \DataTypeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
    \DataTypeTok{data =}\NormalTok{ pca_table}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create confusion matrix}
\NormalTok{confusion_matrix_knn <-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(knn_fit, }\DataTypeTok{positive =} \DecValTok{1}\NormalTok{)}
\NormalTok{confusion_matrix_knn <-}\StringTok{ }\NormalTok{confusion_matrix_knn}\OperatorTok{$}\NormalTok{table }\OperatorTok{%>%}\StringTok{ }\KeywordTok{t}\NormalTok{()}
\NormalTok{confusion_matrix_knn}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          Prediction
## Reference         0         1
##         0 79.010633  1.109570
##         1 17.984281  1.895515
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate confusion matrix scores}
\NormalTok{tn <-}\StringTok{ }\NormalTok{confusion_matrix_knn[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{fn <-}\StringTok{ }\NormalTok{confusion_matrix_knn[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{fp <-}\StringTok{ }\NormalTok{confusion_matrix_knn[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{tp <-}\StringTok{ }\NormalTok{confusion_matrix_knn[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}

\NormalTok{final_}\DecValTok{01}\NormalTok{_loss <-}\StringTok{ }\NormalTok{(fp }\OperatorTok{+}\StringTok{ }\NormalTok{fn) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(acs_outages_class)}
\NormalTok{final_sensitivity <-}\StringTok{ }\NormalTok{tp }\OperatorTok{/}\StringTok{ }\NormalTok{(fn }\OperatorTok{+}\StringTok{ }\NormalTok{tp)}
\NormalTok{final_specificity <-}\StringTok{ }\NormalTok{tn }\OperatorTok{/}\StringTok{ }\NormalTok{(tn }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{final_precision <-}\StringTok{ }\NormalTok{tp }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{final_typeI_error <-}\StringTok{ }\NormalTok{fp }\OperatorTok{/}\StringTok{ }\NormalTok{(tn }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\NormalTok{final_typeII_error <-}\StringTok{ }\NormalTok{fn }\OperatorTok{/}\StringTok{ }\NormalTok{(fn }\OperatorTok{+}\StringTok{ }\NormalTok{tp)}
\NormalTok{final_false_discovery <-}\StringTok{ }\NormalTok{fp }\OperatorTok{/}\StringTok{ }\NormalTok{(tp }\OperatorTok{+}\StringTok{ }\NormalTok{fp)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# confusion matrix comparison}
\NormalTok{confusion_matrix_comparison <-}
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}
    \DataTypeTok{metric =}
      \KeywordTok{c}\NormalTok{(}\StringTok{"Mean 0-1 Loss"}\NormalTok{, }\StringTok{"Precision"}\NormalTok{, }\StringTok{"Sensitivity"}\NormalTok{, }\StringTok{"Specificity"}\NormalTok{,}
        \StringTok{"Type I Error Rate"}\NormalTok{, }\StringTok{"Type II Error Rate"}\NormalTok{, }\StringTok{"False Discovery Rate"}\NormalTok{),}
    \DataTypeTok{base =} 
      \KeywordTok{c}\NormalTok{(base_}\DecValTok{01}\NormalTok{_loss, base_precision, base_sensitivity, base_specificity,}
\NormalTok{        base_typeI_error, base_typeII_error, base_false_discovery),}
    \DataTypeTok{final =} 
      \KeywordTok{c}\NormalTok{(final_}\DecValTok{01}\NormalTok{_loss, final_precision, final_sensitivity, final_specificity,}
\NormalTok{        final_typeI_error, final_typeII_error, final_false_discovery)}
\NormalTok{  ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Above Median Customer Affected Confusion Matrix Metrics"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable_classic}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{#format for markdown}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options=}\StringTok{"scale_down"}\NormalTok{)}
\NormalTok{confusion_matrix_comparison}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-36}Above Median Customer Affected Confusion Matrix Metrics}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r}
\hline
metric & base & final\\
\hline
Mean 0-1 Loss & 0.1927878 & 0.0088275\\
\hline
Precision & 0.6444444 & 0.6307692\\
\hline
Sensitivity & 0.0674419 & 0.0953488\\
\hline
Specificity & 0.9907675 & 0.9861512\\
\hline
Type I Error Rate & 0.0092325 & 0.0138488\\
\hline
Type II Error Rate & 0.9325581 & 0.9046512\\
\hline
False Discovery Rate & 0.3555556 & 0.3692308\\
\hline
\end{tabular}}
\end{table}

Some metrics take a hit (like precision and specificity), but
Sensitivity increased a good deal and the Mean 0-1 Loss improved as well
- which were our choice outcomes. Want the model to detect as many true
positives as possible.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Plot ROC Curve}
\KeywordTok{roc}\NormalTok{(}
  \DataTypeTok{response =}\NormalTok{ pca_table}\OperatorTok{$}\NormalTok{above_median_cust_affected,}
  \DataTypeTok{predictor =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(knn_fit)), }
  \DataTypeTok{data =}\NormalTok{ pca_table, }
  \DataTypeTok{plot =} \OtherTok{TRUE}\NormalTok{,}
  \DataTypeTok{main =} \StringTok{"Final ROC"}\NormalTok{,}
  \DataTypeTok{col =} \StringTok{"blue"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-37-1.pdf}

\begin{verbatim}
## 
## Call:
## roc.default(response = pca_table$above_median_cust_affected,     predictor = as.numeric(predict(knn_fit)), plot = TRUE, data = pca_table,     main = "Final ROC", col = "blue")
## 
## Data: as.numeric(predict(knn_fit)) in 1733 controls (pca_table$above_median_cust_affected 0) < 430 cases (pca_table$above_median_cust_affected 1).
## Area under the curve: 0.548
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# determine AUC}
\KeywordTok{auc}\NormalTok{(}
  \DataTypeTok{response =}\NormalTok{ pca_table}\OperatorTok{$}\NormalTok{above_median_cust_affected,}
  \DataTypeTok{predictor =} \KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{predict}\NormalTok{(knn_fit)),  }
  \DataTypeTok{data =}\NormalTok{ pca_table}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Area under the curve: 0.548
\end{verbatim}

Clearly AUC and ROC take a hit, but this was not the goal of the model.
Wanted to reduce mean 0-1 loss and increase sensitivity above all. This
model is likely biased, but the flag only appears 20\% of the time I
believe such bias is justified. This is especially true because I wanted
to optimize the sensitivity and accuracy of the model.

\newpage

\hypertarget{appendixC}{%
\subsection{Appendix C - Data Processing}\label{appendixC}}

This appendix provides the scripts used in processing, cleaning, and
merging data.

\hypertarget{acs-clean-script}{%
\subsubsection{ACS Clean Script}\label{acs-clean-script}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Setup -------------------------------------------------------------------}
\CommentTok{# Packages:}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{library}\NormalTok{(tigris)}
\KeywordTok{library}\NormalTok{(tidycensus)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{tigris_use_cache =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Directories:}
\NormalTok{homedir <-}\StringTok{ "E:/neighborhood-outages/"}
\NormalTok{workdir <-}\StringTok{ "raw_data/"}
\NormalTok{savedir <-}\StringTok{ "cleaned_data/"}
\KeywordTok{setwd}\NormalTok{(homedir)}

\CommentTok{# Import data:}

\CommentTok{# Parameters:}

\CommentTok{# Main Script -------------------------------------------------------------}

\CommentTok{# Load available variables for 2018 ACS}
\NormalTok{acs_vars <-}\StringTok{ }\KeywordTok{load_variables}\NormalTok{(}\DataTypeTok{year =} \DecValTok{2018}\NormalTok{, }\DataTypeTok{dataset =} \StringTok{"acs5"}\NormalTok{, }\DataTypeTok{cache =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{dec_vars <-}\StringTok{ }\KeywordTok{load_variables}\NormalTok{(}\DataTypeTok{year =} \DecValTok{2010}\NormalTok{, }\DataTypeTok{dataset =} \StringTok{"sf1"}\NormalTok{, }\DataTypeTok{cache =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# load land area for each census tract}
\NormalTok{ca_land_area <-}
\StringTok{  }\KeywordTok{tracts}\NormalTok{(}\StringTok{"CA"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{st_drop_geometry}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, ALAND) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# convert from square meters to square kilometer}
\StringTok{  }\KeywordTok{transmute}\NormalTok{(}
    \DataTypeTok{GEOID =}\NormalTok{ GEOID,}
    \DataTypeTok{land_area_sq_km =}\NormalTok{ ALAND }\OperatorTok{/}\StringTok{ }\DecValTok{1000000}
\NormalTok{  )}

\CommentTok{# Read in ACS data}
\CommentTok{## ACS race data (2018)}
\NormalTok{acs_race <-}
\StringTok{  }\KeywordTok{get_acs}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"tract"}\NormalTok{,}
    \DataTypeTok{table =} \StringTok{"B03002"}\NormalTok{,}
    \DataTypeTok{year =} \DecValTok{2018}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_vars, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{ =}\StringTok{ "name"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, label, estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{# convert names to better form}
    \DataTypeTok{label =} \KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"!!|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{"_"}\NormalTok{)),}
    \CommentTok{# short estimate to est}
    \DataTypeTok{label =} \KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"estimate"}\NormalTok{, }\StringTok{"est"}\NormalTok{),}
    \CommentTok{# remove unnecessary character strings}
    \DataTypeTok{label =} \KeywordTok{str_remove_all}\NormalTok{(label, }\StringTok{"est_total_not_hispanic_or_latino_|_alone"}\NormalTok{)}
\NormalTok{   ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# remove total non-hispanic and hispanic by race}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{str_detect}\NormalTok{(label, }\StringTok{"est_total_not_hispanic_or_latino|or_latino_"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# pivot table}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}
    \DataTypeTok{names_from =}\NormalTok{ label,}
    \DataTypeTok{values_from =}\NormalTok{ estimate}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# rename multiracial category}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{multi_racial =}\NormalTok{ two_or_more_races) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# select out separated multiracial categories}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{starts_with}\NormalTok{(}\StringTok{"two_or_more"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# create proportions by tract}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \KeywordTok{across}\NormalTok{(}
      \KeywordTok{where}\NormalTok{(is.double),}
      \OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{/}\StringTok{ }\NormalTok{est_total,}
      \DataTypeTok{.names =} \StringTok{"prop_\{.col\}"}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# select for just tract ID, total population, and proportions}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, est_total, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"prop"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{prop_est_total) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{prop_latino =}\NormalTok{ prop_est_total_hispanic_or_latino) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# join with land area to create density stat}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(ca_land_area, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{pop_density_sq_km =}\NormalTok{ est_total }\OperatorTok{/}\StringTok{ }\NormalTok{land_area_sq_km) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# drop total population variable}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{est_total)}

\CommentTok{## ACS income data (2018)}
\CommentTok{### Read in median income by tract}
\NormalTok{acs_median_income <-}
\StringTok{  }\KeywordTok{get_acs}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"county"}\NormalTok{,}
    \DataTypeTok{variables =} \StringTok{"B06011_001"}\NormalTok{,}
    \DataTypeTok{year =} \DecValTok{2018}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, }\DataTypeTok{ami =}\NormalTok{ estimate)}

\CommentTok{## ACS income data (2018)}
\NormalTok{acs_income <-}
\StringTok{  }\KeywordTok{get_acs}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"tract"}\NormalTok{,}
    \DataTypeTok{table =} \StringTok{"B19001"}\NormalTok{,}
    \DataTypeTok{year =} \DecValTok{2018}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_vars, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{ =}\StringTok{ "name"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, label, estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{# convert names to better form}
    \DataTypeTok{label =} \KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"!!|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{"_"}\NormalTok{)),}
    \CommentTok{# short estimate to est}
    \DataTypeTok{label =} \KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"estimate"}\NormalTok{, }\StringTok{"est"}\NormalTok{),}
    \DataTypeTok{est_total =} \KeywordTok{if_else}\NormalTok{(label }\OperatorTok{==}\StringTok{ "est_total"}\NormalTok{, estimate, }\OtherTok{NA_real_}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(GEOID) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{fill}\NormalTok{(}\KeywordTok{everything}\NormalTok{(), }\DataTypeTok{.direction =} \StringTok{"down"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(label }\OperatorTok{!=}\StringTok{ "est_total"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{label =} \KeywordTok{str_extract}\NormalTok{(label, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d+,}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d+$"}\NormalTok{),}
    \DataTypeTok{label =} \KeywordTok{str_remove_all}\NormalTok{(label, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{$|[:punct:]"}\NormalTok{),}
    \DataTypeTok{label =} \KeywordTok{if_else}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(label), }\DecValTok{500000}\NormalTok{, }\KeywordTok{as.double}\NormalTok{(label)),}
    \DataTypeTok{county =} \KeywordTok{str_extract}\NormalTok{(GEOID, }\StringTok{"^}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d\{5\}"}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{upper_income_limit =}\NormalTok{ label) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_median_income, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"county"}\NormalTok{ =}\StringTok{ "GEOID"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# construct the income categories}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{income_category =}
      \KeywordTok{case_when}\NormalTok{(}
\NormalTok{        upper_income_limit }\OperatorTok{<=}\StringTok{ }\NormalTok{(.}\DecValTok{30} \OperatorTok{*}\StringTok{ }\NormalTok{ami)       }\OperatorTok{~}\StringTok{ "eli"}\NormalTok{,}
\NormalTok{        upper_income_limit }\OperatorTok{<=}\StringTok{ }\NormalTok{(.}\DecValTok{50} \OperatorTok{*}\StringTok{ }\NormalTok{ami) }\OperatorTok{&}
\StringTok{          }\NormalTok{upper_income_limit }\OperatorTok{>}\StringTok{ }\NormalTok{(.}\DecValTok{30} \OperatorTok{*}\StringTok{ }\NormalTok{ami)      }\OperatorTok{~}\StringTok{ "vli"}\NormalTok{,}
\NormalTok{        upper_income_limit }\OperatorTok{<=}\StringTok{ }\NormalTok{(.}\DecValTok{80} \OperatorTok{*}\StringTok{ }\NormalTok{ami) }\OperatorTok{&}
\StringTok{          }\NormalTok{upper_income_limit }\OperatorTok{>}\StringTok{ }\NormalTok{(.}\DecValTok{50} \OperatorTok{*}\StringTok{ }\NormalTok{ami)      }\OperatorTok{~}\StringTok{ "li"}\NormalTok{,}
\NormalTok{        upper_income_limit }\OperatorTok{<=}\StringTok{ }\NormalTok{(}\FloatTok{1.20} \OperatorTok{*}\StringTok{ }\NormalTok{ami) }\OperatorTok{&}
\StringTok{          }\NormalTok{upper_income_limit }\OperatorTok{>}\StringTok{ }\NormalTok{(.}\DecValTok{80} \OperatorTok{*}\StringTok{ }\NormalTok{ami)      }\OperatorTok{~}\StringTok{ "mi"}\NormalTok{,}
\NormalTok{        upper_income_limit }\OperatorTok{>}\StringTok{ }\NormalTok{(}\FloatTok{1.20} \OperatorTok{*}\StringTok{ }\NormalTok{ami)       }\OperatorTok{~}\StringTok{ "hi"}\NormalTok{,}
        \OtherTok{TRUE}                                    \OperatorTok{~}\StringTok{ }\OtherTok{NA_character_}
\NormalTok{      )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, estimate, income_category, est_total) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(GEOID, income_category) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{estimate =} \KeywordTok{sum}\NormalTok{(estimate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# change shape of data}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ income_category, }\DataTypeTok{values_from =}\NormalTok{ estimate) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# create proportions by tract}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \KeywordTok{across}\NormalTok{(}
\NormalTok{      eli}\OperatorTok{:}\NormalTok{vli,}
      \OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{/}\StringTok{ }\NormalTok{est_total,}
      \DataTypeTok{.names =} \StringTok{"prop_\{.col\}"}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# select for just tract ID, total population, and proportions}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"prop"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\KeywordTok{across}\NormalTok{(}\KeywordTok{where}\NormalTok{(is.double), }\OperatorTok{~}\StringTok{ }\KeywordTok{replace_na}\NormalTok{(.x, }\DataTypeTok{replace =} \DecValTok{0}\NormalTok{)))}

\KeywordTok{rm}\NormalTok{(acs_median_income)}

\CommentTok{## ACS tenure data (2018)}
\NormalTok{acs_tenure <-}
\StringTok{  }\KeywordTok{get_acs}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"tract"}\NormalTok{,}
    \DataTypeTok{table =} \StringTok{"B25003"}\NormalTok{,}
    \DataTypeTok{year =} \DecValTok{2018}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_vars, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{ =}\StringTok{ "name"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, label, estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{# convert names to better form}
    \DataTypeTok{label =} \KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"!!|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{"_"}\NormalTok{)),}
    \CommentTok{# short estimate to est}
    \DataTypeTok{label =} \KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"estimate"}\NormalTok{, }\StringTok{"est"}\NormalTok{),}
    \DataTypeTok{est_total =} \KeywordTok{if_else}\NormalTok{(label }\OperatorTok{==}\StringTok{ "est_total"}\NormalTok{, estimate, }\OtherTok{NA_real_}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(GEOID) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{fill}\NormalTok{(}\KeywordTok{everything}\NormalTok{(), }\DataTypeTok{.direction =} \StringTok{"down"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(label }\OperatorTok{!=}\StringTok{ "est_total"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{label =} \KeywordTok{str_remove_all}\NormalTok{(label, }\StringTok{"est_total_|_occupied"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ label, }\DataTypeTok{values_from =}\NormalTok{ estimate) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# create proportions by tract}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \KeywordTok{across}\NormalTok{(}
\NormalTok{      owner}\OperatorTok{:}\NormalTok{renter,}
      \OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{/}\StringTok{ }\NormalTok{est_total,}
      \DataTypeTok{.names =} \StringTok{"prop_\{.col\}"}
\NormalTok{    )}
\NormalTok{  )}

\CommentTok{## ACS college-educated data (2018)}
\NormalTok{acs_education <-}
\StringTok{  }\KeywordTok{get_acs}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"tract"}\NormalTok{,}
    \DataTypeTok{table =} \StringTok{"B15003"}\NormalTok{,}
    \DataTypeTok{year =} \DecValTok{2018}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_vars, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{ =}\StringTok{ "name"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, label, estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{# convert names to better form}
    \DataTypeTok{label =} \KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"!!|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{"_"}\NormalTok{)),}
    \CommentTok{# short estimate to est}
    \DataTypeTok{label =} \KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"estimate"}\NormalTok{, }\StringTok{"est"}\NormalTok{),}
    \DataTypeTok{est_total =} \KeywordTok{if_else}\NormalTok{(label }\OperatorTok{==}\StringTok{ "est_total"}\NormalTok{, estimate, }\OtherTok{NA_real_}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(GEOID) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{fill}\NormalTok{(}\KeywordTok{everything}\NormalTok{(), }\DataTypeTok{.direction =} \StringTok{"down"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(label }\OperatorTok{!=}\StringTok{ "est_total"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{label =} \KeywordTok{str_remove_all}\NormalTok{(label, }\StringTok{"est_total_"}\NormalTok{),}
    \DataTypeTok{label =}
      \KeywordTok{case_when}\NormalTok{(}
        \KeywordTok{str_detect}\NormalTok{(}
\NormalTok{          label, }\StringTok{"associate|bachelor|master|professional|doctorate"}
\NormalTok{        )                                                           }\OperatorTok{~}\StringTok{ "college"}\NormalTok{,}
        \KeywordTok{str_detect}\NormalTok{(}
\NormalTok{          label, }\StringTok{"high_school|ged_or_|some_college"}
\NormalTok{        )                                                       }\OperatorTok{~}\StringTok{ "high_school"}\NormalTok{,}
        \OtherTok{TRUE}                                                    \OperatorTok{~}\StringTok{ "less_than_hs"}
\NormalTok{      )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(GEOID, label) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{estimate =} \KeywordTok{sum}\NormalTok{(estimate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ label, }\DataTypeTok{values_from =}\NormalTok{ estimate) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# create proportions by tract}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \KeywordTok{across}\NormalTok{(}
\NormalTok{      college}\OperatorTok{:}\NormalTok{less_than_hs,}
      \OperatorTok{~}\StringTok{ }\NormalTok{. }\OperatorTok{/}\StringTok{ }\NormalTok{est_total,}
      \DataTypeTok{.names =} \StringTok{"prop_\{.col\}"}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# select for just tract ID, total population, and proportions}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"prop"}\NormalTok{))}

\CommentTok{## ACS vacancy data (2018)}
\NormalTok{acs_vacancy <-}
\StringTok{  }\KeywordTok{get_acs}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"tract"}\NormalTok{,}
    \DataTypeTok{table =} \StringTok{"B25004"}\NormalTok{,}
    \DataTypeTok{year =} \DecValTok{2018}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_vars, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{ =}\StringTok{ "name"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, label, estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{# convert names to better form}
    \DataTypeTok{label =} \KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"!!|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{"_"}\NormalTok{)),}
    \CommentTok{# short estimate to est}
    \DataTypeTok{label =} \KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"estimate"}\NormalTok{, }\StringTok{"est"}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}
\NormalTok{    label }\OperatorTok{%in%}
\StringTok{      }\KeywordTok{c}\NormalTok{(}
        \StringTok{"est_total_for_rent"}\NormalTok{, }\StringTok{"est_total_rented,_not_occupied"}\NormalTok{,}
        \StringTok{"est_total_for_sale_only"}\NormalTok{, }\StringTok{"est_total_sold,_not_occupied"}
\NormalTok{      )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{label =} \KeywordTok{str_remove_all}\NormalTok{(label, }\StringTok{"est_total_|,"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ label, }\DataTypeTok{values_from =}\NormalTok{ estimate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_tenure, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{transmute}\NormalTok{(}
    \DataTypeTok{GEOID =}\NormalTok{ GEOID,}
    \DataTypeTok{rental_vacancy_rate =}\NormalTok{ for_rent }\OperatorTok{/}\StringTok{ }\NormalTok{(renter }\OperatorTok{+}\StringTok{ }\NormalTok{rented_not_occupied }\OperatorTok{+}\StringTok{ }\NormalTok{for_rent),}
    \DataTypeTok{owner_vacancy_rate =}\NormalTok{ for_sale_only }\OperatorTok{/}\StringTok{ }\NormalTok{(owner }\OperatorTok{+}\StringTok{ }\NormalTok{sold_not_occupied }\OperatorTok{+}\StringTok{ }\NormalTok{for_sale_only)}
\NormalTok{  )}

\CommentTok{### Update tenure}
\NormalTok{acs_tenure <-}
\StringTok{  }\NormalTok{acs_tenure }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# select for just tract ID, total population, and proportions}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, }\KeywordTok{starts_with}\NormalTok{(}\StringTok{"prop"}\NormalTok{))}

\CommentTok{## Urban and rural population}
\NormalTok{dec_urban <-}
\StringTok{  }\KeywordTok{get_decennial}\NormalTok{(}
    \DataTypeTok{geography =} \StringTok{"tract"}\NormalTok{,}
    \DataTypeTok{variables =} \KeywordTok{c}\NormalTok{(}\StringTok{"P002001"}\NormalTok{, }\StringTok{"P002002"}\NormalTok{, }\StringTok{"P002005"}\NormalTok{),}
    \DataTypeTok{year =} \DecValTok{2010}\NormalTok{,}
    \DataTypeTok{cache_table =} \OtherTok{TRUE}\NormalTok{,}
    \DataTypeTok{state =} \StringTok{"CA"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(dec_vars, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"variable"}\NormalTok{ =}\StringTok{ "name"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, label, value) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{# convert names to better form}
    \DataTypeTok{label =} \KeywordTok{str_to_lower}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(label, }\StringTok{"!!|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+"}\NormalTok{, }\StringTok{"_"}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{(}\DataTypeTok{names_from =}\NormalTok{ label, }\DataTypeTok{values_from =}\NormalTok{ value) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{prop_urban =}\NormalTok{ total_urban }\OperatorTok{/}\StringTok{ }\NormalTok{total,}
    \DataTypeTok{prop_rural =}\NormalTok{ total_rural }\OperatorTok{/}\StringTok{ }\NormalTok{total}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(GEOID, prop_rural, prop_urban)}

\CommentTok{# Merge all ACS data}
\NormalTok{acs_merged <-}
\StringTok{  }\NormalTok{acs_race }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_income, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_education, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_tenure, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs_vacancy, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(dec_urban)}

\CommentTok{# Save Results ------------------------------------------------------------}
\KeywordTok{write_csv}\NormalTok{(}
\NormalTok{  acs_merged,}
  \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(homedir, savedir, }\StringTok{"acs_clean.csv"}\NormalTok{)}
\NormalTok{)}

\KeywordTok{rm}\NormalTok{(}
\NormalTok{  acs_education, acs_income, acs_merged, acs_race, acs_tenure,}
\NormalTok{  acs_vacancy, acs_vars, ca_land_area, dec_urban, dec_vars}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{outage-clean-script}{%
\subsubsection{Outage Clean Script}\label{outage-clean-script}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Setup -------------------------------------------------------------------}
\CommentTok{# Packages:}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{library}\NormalTok{(tigris)}
\KeywordTok{library}\NormalTok{(lubridate)}
\KeywordTok{options}\NormalTok{(}\DataTypeTok{tigris_use_cache =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Directories:}
\NormalTok{homedir <-}\StringTok{ "E:/neighborhood-outages/"}
\NormalTok{workdir <-}\StringTok{ "raw_data/"}
\NormalTok{savedir <-}\StringTok{ "cleaned_data/"}
\KeywordTok{setwd}\NormalTok{(homedir)}

\CommentTok{# Import data:}
\NormalTok{outages <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(homedir, workdir, }\StringTok{"outages_expanded.csv"}\NormalTok{))}

\CommentTok{# Parameters:}

\CommentTok{# Main Script -------------------------------------------------------------}

\CommentTok{# Read in California block groups}
\NormalTok{ca_tracts <-}\StringTok{ }\KeywordTok{tracts}\NormalTok{(}\DataTypeTok{state =} \StringTok{"CA"}\NormalTok{)}

\CommentTok{# Filter outages and geocode lat/long to Census tract}
\NormalTok{outages_filter <-}
\StringTok{  }\NormalTok{outages }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# Extract possible duration hours, min/max est affected, lat/long}
\StringTok{  }\KeywordTok{select}\NormalTok{(}
\NormalTok{    possible_duration_hours,}
\NormalTok{    min_estCustAffected,}
\NormalTok{    max_estCustAffected,}
\NormalTok{    latitude,}
\NormalTok{    longitude}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# determine mean customers affected}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{mean_cust_affected =}\NormalTok{ (min_estCustAffected }\OperatorTok{+}\StringTok{ }\NormalTok{max_estCustAffected) }\OperatorTok{/}\StringTok{ }\DecValTok{2}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# convert to sf object}
\StringTok{  }\KeywordTok{st_as_sf}\NormalTok{(}
    \DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"longitude"}\NormalTok{, }\StringTok{"latitude"}\NormalTok{), }\DataTypeTok{crs =} \KeywordTok{st_crs}\NormalTok{(ca_tracts)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# join to CA block groups}
\StringTok{  }\KeywordTok{st_join}\NormalTok{(ca_tracts) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# drop geometry}
\StringTok{  }\KeywordTok{st_drop_geometry}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# select out unnecessary columns}
\StringTok{  }\KeywordTok{select}\NormalTok{(}
\NormalTok{    GEOID, }\DataTypeTok{outage_duration_hr =}\NormalTok{ possible_duration_hours, mean_cust_affected}
\NormalTok{  )}

\CommentTok{# Remove unnecessary objects}
\KeywordTok{rm}\NormalTok{(ca_tracts, outages)}

\CommentTok{# Save Results ------------------------------------------------------------}
\KeywordTok{write_csv}\NormalTok{(}
\NormalTok{  outages_filter,}
  \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(homedir, savedir, }\StringTok{"outages_clean.csv"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{acs-and-outage-merge-script}{%
\subsubsection{ACS and Outage Merge
Script}\label{acs-and-outage-merge-script}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Setup -------------------------------------------------------------------}
\CommentTok{# Packages:}
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#library(here)}

\CommentTok{# Directories:}
\NormalTok{homedir <-}\StringTok{ "E:/neighborhood-outages/"}
\NormalTok{workdir <-}\StringTok{ "cleaned_data/"}
\NormalTok{savedir <-}\StringTok{ "cleaned_data/"}
\KeywordTok{setwd}\NormalTok{(homedir)}

\CommentTok{# Parameters}
\NormalTok{outages_filepath <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(homedir, workdir, }\StringTok{"outages_clean.csv"}\NormalTok{)}
\NormalTok{acs_filepath <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(homedir, workdir, }\StringTok{"acs_clean.csv"}\NormalTok{)}

\CommentTok{#outages_filepath <- here("cleaned_data/outages_clean.csv")}
\CommentTok{#acs_filepath <- here("cleaned_data/acs_clean.csv")}

\CommentTok{# Import data:}
\NormalTok{outages <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(outages_filepath)}
\NormalTok{acs <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(acs_filepath)}

\CommentTok{# Parameters:}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{572}\NormalTok{)}

\CommentTok{# Main Script -------------------------------------------------------------}

\NormalTok{outages_grouped <-}
\StringTok{  }\NormalTok{outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(GEOID) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{median_outage_duration_hr =} \KeywordTok{median}\NormalTok{(outage_duration_hr),}
    \DataTypeTok{median_mean_cust_affected =} \KeywordTok{median}\NormalTok{(mean_cust_affected),}
    \DataTypeTok{num_outages =} \KeywordTok{n}\NormalTok{()}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{above_median_cust_affected =}
      \KeywordTok{if_else}\NormalTok{(}
\NormalTok{        median_mean_cust_affected }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(median_mean_cust_affected), }\DecValTok{1}\NormalTok{, }\DecValTok{0}
\NormalTok{      )}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# remove median of mean customers affected column}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{median_mean_cust_affected)}

\NormalTok{acs_outages <-}
\StringTok{  }\NormalTok{outages_grouped }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{left_join}\NormalTok{(acs, }\DataTypeTok{by =} \StringTok{"GEOID"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{median_outage_duration_hr =} \KeywordTok{replace_na}\NormalTok{(median_outage_duration_hr, }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{above_median_cust_affected =} \KeywordTok{replace_na}\NormalTok{(above_median_cust_affected, }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{num_outages =} \KeywordTok{replace_na}\NormalTok{(num_outages, }\DecValTok{0}\NormalTok{),}
    \DataTypeTok{n_outages_sq_km =}\NormalTok{ num_outages }\OperatorTok{/}\StringTok{ }\NormalTok{land_area_sq_km}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# drop unneeded variables}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\KeywordTok{c}\NormalTok{(num_outages, land_area_sq_km)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rowid =} \KeywordTok{row_number}\NormalTok{())}

\CommentTok{# CA census tracts that we don't have outage data for, as they are likely}
\CommentTok{# not serviced by PG&E.}
\NormalTok{non_outage_tracts <-}
\StringTok{  }\KeywordTok{setdiff}\NormalTok{(acs}\OperatorTok{$}\NormalTok{GEOID, outages_grouped}\OperatorTok{$}\NormalTok{GEOID) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{non_pge_tract =}\NormalTok{ value)}

\CommentTok{# Create test and train sets}

\NormalTok{acs_outages_test <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{slice_sample}\NormalTok{(}\DataTypeTok{prop =} \FloatTok{.2}\NormalTok{)}

\NormalTok{acs_outages_train <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\NormalTok{(rowid }\OperatorTok{%in%}\StringTok{ }\NormalTok{acs_outages_test}\OperatorTok{$}\NormalTok{rowid)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{rowid)}

\NormalTok{acs_outages_test <-}\StringTok{ }\NormalTok{acs_outages_test }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{rowid)}

\KeywordTok{rm}\NormalTok{(acs, outages, outages_grouped, acs_outages)}

\CommentTok{# Save Results ------------------------------------------------------------}
\CommentTok{## write test ACS outages data}
\KeywordTok{write_csv}\NormalTok{(}
\NormalTok{  acs_outages_test,}
  \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(homedir, savedir, }\StringTok{"acs_outages_test.csv"}\NormalTok{)}
\NormalTok{)}

\CommentTok{## write train ACS outages data}
\KeywordTok{write_csv}\NormalTok{(}
\NormalTok{  acs_outages_train,}
  \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(homedir, savedir, }\StringTok{"acs_outages_train.csv"}\NormalTok{)}
\NormalTok{)}

\CommentTok{## write non-outage tracts}
\KeywordTok{write_csv}\NormalTok{(}
\NormalTok{  non_outage_tracts,}
  \DataTypeTok{file =} \KeywordTok{paste0}\NormalTok{(homedir, savedir, }\StringTok{"non_outage_tracts.csv"}\NormalTok{)}
\NormalTok{)}

\KeywordTok{rm}\NormalTok{(non_outage_tracts, acs_outages_test, acs_outages_train)}
\end{Highlighting}
\end{Shaded}

\newpage

\hypertarget{appendixD}{%
\subsection{Appendix D - EDA}\label{appendixD}}

This appendix provides all the EDA conducted prior to model building.

\hypertarget{missing-values}{%
\subsubsection{Missing values}\label{missing-values}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# NA values?}
\CommentTok{# Filter all the columns to exclude NA}
\NormalTok{no_na_df <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{across}\NormalTok{(}\KeywordTok{everything}\NormalTok{(), }\OperatorTok{~}\StringTok{ }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(.)))}

\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\NormalTok{(GEOID }\OperatorTok{%in%}\StringTok{ }\NormalTok{no_na_df}\OperatorTok{$}\NormalTok{GEOID)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(prop_white))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 15 x 27
##    GEOID median_outage_d~ above_median_cu~ prop_white prop_black_or_a~
##    <chr>            <dbl>            <dbl>      <dbl>            <dbl>
##  1 0600~             0.67                1      0.883           0     
##  2 0607~             0.83                0      0.833           0     
##  3 0608~             1.66                0      0.641           0.172 
##  4 0605~             3                   1      0.629           0.0443
##  5 0605~             2.5                 0      0.548           0.220 
##  6 0609~             3                   0      0.548           0.186 
##  7 0611~            36.7                 0      0.539           0.0847
##  8 0603~             3.24                1      0.485           0.173 
##  9 0605~             9.76                0      0.428           0.0544
## 10 0607~             1.33                0      0.403           0.339 
## 11 0607~             1.33                0      0.402           0.0565
## 12 0607~             2.17                0      0.299           0.0787
## 13 0601~             2.5                 0      0.217           0.117 
## 14 0607~             4.67                1      0.194           0.509 
## 15 0609~             9.75                0     NA              NA     
## # ... with 22 more variables: prop_american_indian_and_alaska_native <dbl>,
## #   prop_asian <dbl>, prop_native_hawaiian_and_other_pacific_islander <dbl>,
## #   prop_some_other_race <dbl>, prop_multi_racial <dbl>, prop_latino <dbl>,
## #   pop_density_sq_km <dbl>, prop_eli <dbl>, prop_hi <dbl>, prop_li <dbl>,
## #   prop_mi <dbl>, prop_vli <dbl>, prop_college <dbl>, prop_high_school <dbl>,
## #   prop_less_than_hs <dbl>, prop_owner <dbl>, prop_renter <dbl>,
## #   rental_vacancy_rate <dbl>, owner_vacancy_rate <dbl>, prop_rural <dbl>,
## #   prop_urban <dbl>, n_outages_sq_km <dbl>
\end{verbatim}

Not that many NAs, should consider dropping.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# NULL values?}
\CommentTok{# Filter all the columns to exclude NA}
\NormalTok{no_null_df <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{across}\NormalTok{(}\KeywordTok{everything}\NormalTok{(), }\OperatorTok{~}\StringTok{ }\OperatorTok{!}\KeywordTok{is.null}\NormalTok{(.)))}

\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\NormalTok{(GEOID }\OperatorTok{%in%}\StringTok{ }\NormalTok{no_null_df}\OperatorTok{$}\NormalTok{GEOID)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(prop_white))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 0 x 27
## # ... with 27 variables: GEOID <chr>, median_outage_duration_hr <dbl>,
## #   above_median_cust_affected <dbl>, prop_white <dbl>,
## #   prop_black_or_african_american <dbl>,
## #   prop_american_indian_and_alaska_native <dbl>, prop_asian <dbl>,
## #   prop_native_hawaiian_and_other_pacific_islander <dbl>,
## #   prop_some_other_race <dbl>, prop_multi_racial <dbl>, prop_latino <dbl>,
## #   pop_density_sq_km <dbl>, prop_eli <dbl>, prop_hi <dbl>, prop_li <dbl>,
## #   prop_mi <dbl>, prop_vli <dbl>, prop_college <dbl>, prop_high_school <dbl>,
## #   prop_less_than_hs <dbl>, prop_owner <dbl>, prop_renter <dbl>,
## #   rental_vacancy_rate <dbl>, owner_vacancy_rate <dbl>, prop_rural <dbl>,
## #   prop_urban <dbl>, n_outages_sq_km <dbl>
\end{verbatim}

No NULL values.

\hypertarget{correlations}{%
\subsubsection{Correlations}\label{correlations}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Generate correlation matrices with p-values }\AlertTok{###}

\CommentTok{# remove continuous/outcome vars if sd < 0}
\NormalTok{corr_matrix <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(continuous_vars), }\KeywordTok{all_of}\NormalTok{(outcome_vars)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{where}\NormalTok{(}\OperatorTok{~}\KeywordTok{sd}\NormalTok{(., }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{))}

\CommentTok{# create correlation matrix}
\NormalTok{corr_matrix <-}\StringTok{ }\KeywordTok{rcorr}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(corr_matrix))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in sqrt(1 - h * h): NaNs produced
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corr_matrix <-}
\StringTok{  }\CommentTok{# merge p-values}
\StringTok{  }\KeywordTok{bind_cols}\NormalTok{(}
    \KeywordTok{as.data.frame}\NormalTok{(corr_matrix}\OperatorTok{$}\NormalTok{r),}
    \KeywordTok{as.data.frame}\NormalTok{(corr_matrix}\OperatorTok{$}\NormalTok{P) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{rename_all}\NormalTok{(}\OperatorTok{~}\KeywordTok{paste0}\NormalTok{(.x, }\StringTok{"_p-value"}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# filter to relevant predictor and outcome variables}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), }\KeywordTok{all_of}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(outcome_vars, }\StringTok{"_p-value"}\NormalTok{))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{rownames_to_column}\NormalTok{(}\DataTypeTok{var =} \StringTok{"predictor"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}
\NormalTok{    predictor }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(continuous_vars, }\KeywordTok{paste0}\NormalTok{(continuous_vars, }\StringTok{"_p-value"}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{column_to_rownames}\NormalTok{(}\DataTypeTok{var =} \StringTok{"predictor"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{sort}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(.))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{### Filter to highest correlated & statistical significant }\AlertTok{###}
\CommentTok{# For each outcome variable, select significant correlations (in abs value)}

\CommentTok{# create list of variables to use as a filter}
\NormalTok{corr_top <-}\StringTok{ }\KeywordTok{foreach}\NormalTok{(}\DataTypeTok{out =}\NormalTok{ outcome_vars) }\OperatorTok{%do%}\StringTok{ }\NormalTok{\{}
\NormalTok{  out_p <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(out, }\StringTok{"_p-value"}\NormalTok{)}
\NormalTok{  corr_matrix }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{!!}\NormalTok{out, }\OperatorTok{!!}\NormalTok{out_p) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!!}\KeywordTok{sym}\NormalTok{(out_p) }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(}\KeywordTok{abs}\NormalTok{(.))) }\OperatorTok{%>%}
\StringTok{    }\CommentTok{# slice to the top 10 results}
\StringTok{    }\CommentTok{# head(10) %>%}
\StringTok{    }\CommentTok{# extract the rownames for filtering}
\StringTok{    }\KeywordTok{rename_all}\NormalTok{(}\OperatorTok{~}\KeywordTok{str_remove}\NormalTok{(., }\StringTok{"outcome_"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{rownames_to_column}\NormalTok{(}\DataTypeTok{var =} \StringTok{"predictor"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{predictor =} \KeywordTok{str_remove}\NormalTok{(predictor, }\StringTok{"continuous_"}\NormalTok{))}
\NormalTok{\}}
\KeywordTok{names}\NormalTok{(corr_top) <-}\StringTok{ }\KeywordTok{str_remove}\NormalTok{(outcome_vars, }\StringTok{"outcome_"}\NormalTok{)}

\NormalTok{corr_top}\OperatorTok{$}\NormalTok{median_outage_duration_hr }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Median Outage Duration Correlations"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable_classic}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\CommentTok{#format for markdown}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options=}\StringTok{"scale_down"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-45}Median Outage Duration Correlations}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r}
\hline
predictor & median\_outage\_duration\_hr & median\_outage\_duration\_hr\_p-value\\
\hline
n\_outages\_sq\_km & -0.1123572 & 0.0000001\\
\hline
prop\_white & 0.1120240 & 0.0000002\\
\hline
prop\_american\_indian\_and\_alaska\_native & 0.1059761 & 0.0000007\\
\hline
pop\_density\_sq\_km & -0.0886237 & 0.0000345\\
\hline
prop\_latino & -0.0880026 & 0.0000393\\
\hline
prop\_less\_than\_hs & -0.0833460 & 0.0000989\\
\hline
prop\_urban & -0.0664365 & 0.0019211\\
\hline
prop\_rural & 0.0664365 & 0.0019211\\
\hline
prop\_black\_or\_african\_american & -0.0621140 & 0.0037403\\
\hline
prop\_owner & 0.0579319 & 0.0068824\\
\hline
prop\_renter & -0.0579319 & 0.0068824\\
\hline
prop\_eli & -0.0551512 & 0.0100433\\
\hline
owner\_vacancy\_rate & 0.0466866 & 0.0299138\\
\hline
prop\_college & 0.0453477 & 0.0343685\\
\hline
\end{tabular}}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corr_top}\OperatorTok{$}\NormalTok{above_median_cust_affected }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{"Above Median Customer Affected Correlations"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{kableExtra}\OperatorTok{::}\KeywordTok{kable_classic}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\CommentTok{#format for markdown}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options=}\StringTok{"scale_down"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:unnamed-chunk-46}Above Median Customer Affected Correlations}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r}
\hline
predictor & above\_median\_cust\_affected & above\_median\_cust\_affected\_p-value\\
\hline
prop\_white & 0.2244869 & 0.0000000\\
\hline
prop\_latino & -0.1981403 & 0.0000000\\
\hline
prop\_urban & -0.1745335 & 0.0000000\\
\hline
prop\_rural & 0.1745335 & 0.0000000\\
\hline
prop\_less\_than\_hs & -0.1662176 & 0.0000000\\
\hline
n\_outages\_sq\_km & -0.1380306 & 0.0000000\\
\hline
prop\_american\_indian\_and\_alaska\_native & 0.1175222 & 0.0000000\\
\hline
prop\_owner & 0.1136678 & 0.0000001\\
\hline
prop\_renter & -0.1136678 & 0.0000001\\
\hline
prop\_college & 0.1084843 & 0.0000004\\
\hline
pop\_density\_sq\_km & -0.0946454 & 0.0000097\\
\hline
prop\_black\_or\_african\_american & -0.0870522 & 0.0000476\\
\hline
rental\_vacancy\_rate & 0.0813124 & 0.0001469\\
\hline
prop\_hi & 0.0651032 & 0.0023675\\
\hline
prop\_li & -0.0644803 & 0.0026070\\
\hline
owner\_vacancy\_rate & 0.0548375 & 0.0107467\\
\hline
prop\_eli & -0.0466111 & 0.0296128\\
\hline
prop\_mi & -0.0446051 & 0.0373865\\
\hline
\end{tabular}}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create ggpairs scatterplot - looking at top 10 correlations}
\CommentTok{# visualize data to choose transformation}


\CommentTok{# make a function to plot generic data with points and a loess line}
\NormalTok{my_fn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, mapping, }\DataTypeTok{method=}\StringTok{"loess"}\NormalTok{, ...)\{}
\NormalTok{  p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{mapping =}\NormalTok{ mapping) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \FloatTok{0.01}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\NormalTok{method, }\DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\NormalTok{x, ...)}
  \KeywordTok{return}\NormalTok{(p)}
\NormalTok{\}}

\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{GEOID) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# filter to significantly correlated variables}
\StringTok{  }\KeywordTok{select}\NormalTok{(}
    \KeywordTok{all_of}\NormalTok{(outcome_vars),}
\NormalTok{    corr_top}\OperatorTok{$}\NormalTok{median_outage_duration_hr}\OperatorTok{$}\NormalTok{predictor }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{),}
\NormalTok{    corr_top}\OperatorTok{$}\NormalTok{above_median_cust_affected}\OperatorTok{$}\NormalTok{predictor }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggpairs}\NormalTok{(}
    \DataTypeTok{upper =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous =} \KeywordTok{wrap}\NormalTok{(}\StringTok{"cor"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{), }\DataTypeTok{size =} \FloatTok{0.01}\NormalTok{),}
    \DataTypeTok{lower =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{continuous =}\NormalTok{ my_fn, }\DataTypeTok{combo =} \KeywordTok{wrap}\NormalTok{(}\StringTok{"facethist"}\NormalTok{, }\DataTypeTok{binwidth =} \FloatTok{.1}\NormalTok{)),}
    \DataTypeTok{progress =} \OtherTok{FALSE}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_grey}\NormalTok{(}\DataTypeTok{base_size =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-47-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find variables most correlated to each other}
\CommentTok{# (select correlations w/ abs > .5)}
\NormalTok{covariate_corr <-}
\StringTok{  }\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(continuous_vars), }\KeywordTok{all_of}\NormalTok{(outcome_vars)) }\OperatorTok{%>%}
\StringTok{  }\CommentTok{# drop prop owner}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{prop_owner) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{where}\NormalTok{(}\OperatorTok{~}\KeywordTok{sd}\NormalTok{(., }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{correlate}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{stretch}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(r) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{abs}\NormalTok{(r) }\OperatorTok{>}\StringTok{ }\FloatTok{.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Correlation method: 'pearson'
## Missing treated using: 'pairwise.complete.obs'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# reformat in formula form}
\NormalTok{interaction_vars <-}
\StringTok{  }\NormalTok{covariate_corr }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{lead_y =} \KeywordTok{lead}\NormalTok{(y)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(x }\OperatorTok{!=}\StringTok{ }\NormalTok{lead_y) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(x, y) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unite}\NormalTok{(}\DataTypeTok{col =} \StringTok{"interact"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{":"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unlist}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{paste}\NormalTok{(}\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)}
\NormalTok{covariate_corr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 30 x 3
##    x                 y                      r
##    <chr>             <chr>              <dbl>
##  1 prop_rural        prop_urban        -1    
##  2 prop_urban        prop_rural        -1    
##  3 prop_college      prop_less_than_hs -0.790
##  4 prop_less_than_hs prop_college      -0.790
##  5 prop_college      prop_high_school  -0.785
##  6 prop_high_school  prop_college      -0.785
##  7 prop_latino       prop_college      -0.761
##  8 prop_college      prop_latino       -0.761
##  9 prop_hi           prop_vli          -0.741
## 10 prop_vli          prop_hi           -0.741
## # ... with 20 more rows
\end{verbatim}

\hypertarget{median-outage-histogram}{%
\subsubsection{Median outage histogram}\label{median-outage-histogram}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-49-1.pdf}

\hypertarget{above-average-customer-affected-distribution-plot}{%
\subsubsection{Above average customer affected distribution
plot}\label{above-average-customer-affected-distribution-plot}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.factor}\NormalTok{(above_median_cust_affected))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_bar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ (..count..)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(..count..))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Above Median Customers Affected"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Proportion of Data"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-50-1.pdf}

\hypertarget{bar-plots}{%
\subsubsection{Bar plots}\label{bar-plots}}

\hypertarget{race}{%
\paragraph{Race}\label{race}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_white}\OperatorTok{:}\NormalTok{prop_latino,}
    \DataTypeTok{names_to =} \StringTok{"race"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_race"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{race =}\KeywordTok{str_to_title}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(race, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), race, prop_race) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr, }\DataTypeTok{y =}\NormalTok{ prop_race, }\DataTypeTok{color =}\NormalTok{ race)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-51-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_white}\OperatorTok{:}\NormalTok{prop_latino,}
    \DataTypeTok{names_to =} \StringTok{"race"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_race"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{race =}\KeywordTok{str_to_title}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(race, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), race, prop_race) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(above_median_cust_affected, race) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{prop_race_mean =} \KeywordTok{mean}\NormalTok{(prop_race, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{as_factor}\NormalTok{(above_median_cust_affected),}
      \DataTypeTok{y =}\NormalTok{ prop_race_mean,}
      \DataTypeTok{fill =}\NormalTok{ race}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `summarise()` regrouping output by 'above_median_cust_affected' (override with `.groups` argument)
\end{verbatim}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-52-1.pdf}

\hypertarget{population-density}{%
\paragraph{Population density}\label{population-density}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr, }\DataTypeTok{y =}\NormalTok{ pop_density_sq_km)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-53-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{as_factor}\NormalTok{(above_median_cust_affected),}
      \DataTypeTok{y =}\NormalTok{ pop_density_sq_km}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-54-1.pdf}

\hypertarget{ami-groups}{%
\paragraph{AMI Groups}\label{ami-groups}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_eli}\OperatorTok{:}\NormalTok{prop_vli,}
    \DataTypeTok{names_to =} \StringTok{"ami_group"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_ami_group"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{ami_group =}
      \KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(ami_group, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), ami_group, prop_ami_group) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr, }\DataTypeTok{y =}\NormalTok{ prop_ami_group, }\DataTypeTok{color =}\NormalTok{ ami_group)}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-55-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{ }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_eli}\OperatorTok{:}\NormalTok{prop_vli,}
    \DataTypeTok{names_to =} \StringTok{"ami_group"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_ami_group"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{race =}
      \KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(ami_group, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), ami_group, prop_ami_group) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(above_median_cust_affected, ami_group) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{prop_ami_mean =} \KeywordTok{mean}\NormalTok{(prop_ami_group, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{as_factor}\NormalTok{(above_median_cust_affected),}
      \DataTypeTok{y =}\NormalTok{ prop_ami_mean,}
      \DataTypeTok{fill =}\NormalTok{ ami_group}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-56-1.pdf}

\hypertarget{education}{%
\paragraph{Education}\label{education}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_college}\OperatorTok{:}\NormalTok{prop_less_than_hs,}
    \DataTypeTok{names_to =} \StringTok{"education"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_education"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{education =}
      \KeywordTok{str_to_title}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(education, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), education, prop_education) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr, }\DataTypeTok{y =}\NormalTok{ prop_education, }\DataTypeTok{color =}\NormalTok{ education)}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-57-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_college}\OperatorTok{:}\NormalTok{prop_less_than_hs,}
    \DataTypeTok{names_to =} \StringTok{"education"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_education"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{education =}
      \KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(education, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), education, prop_education) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(above_median_cust_affected, education) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{prop_education_mean =} \KeywordTok{mean}\NormalTok{(prop_education, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{as_factor}\NormalTok{(above_median_cust_affected),}
      \DataTypeTok{y =}\NormalTok{ prop_education_mean,}
      \DataTypeTok{fill =}\NormalTok{ education}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-58-1.pdf}

\hypertarget{tenure}{%
\paragraph{Tenure}\label{tenure}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_owner}\OperatorTok{:}\NormalTok{prop_renter,}
    \DataTypeTok{names_to =} \StringTok{"tenure"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_tenure"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{tenure =}
      \KeywordTok{str_to_title}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(tenure, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), tenure, prop_tenure) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr, }\DataTypeTok{y =}\NormalTok{ prop_tenure, }\DataTypeTok{color =}\NormalTok{ tenure)}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-59-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ prop_owner}\OperatorTok{:}\NormalTok{prop_renter,}
    \DataTypeTok{names_to =} \StringTok{"tenure"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"prop_tenure"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{tenure =}
      \KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(tenure, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), tenure, prop_tenure) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(above_median_cust_affected, tenure) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{prop_tenure_mean =} \KeywordTok{mean}\NormalTok{(prop_tenure, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{as_factor}\NormalTok{(above_median_cust_affected),}
      \DataTypeTok{y =}\NormalTok{ prop_tenure_mean,}
      \DataTypeTok{fill =}\NormalTok{ tenure}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-60-1.pdf}

\hypertarget{vacancy-rates}{%
\paragraph{Vacancy rates}\label{vacancy-rates}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ rental_vacancy_rate}\OperatorTok{:}\NormalTok{owner_vacancy_rate,}
    \DataTypeTok{names_to =} \StringTok{"vacancy_type"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"vacancy_rate"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{vacancy_type =}
      \KeywordTok{str_to_title}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(vacancy_type, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), vacancy_type, vacancy_rate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ median_outage_duration_hr, }\DataTypeTok{y =}\NormalTok{ vacancy_rate, }\DataTypeTok{color =}\NormalTok{ vacancy_type)}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-61-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs_outages }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(}
    \DataTypeTok{cols =}\NormalTok{ rental_vacancy_rate}\OperatorTok{:}\NormalTok{owner_vacancy_rate,}
    \DataTypeTok{names_to =} \StringTok{"vacancy_type"}\NormalTok{,}
    \DataTypeTok{values_to =} \StringTok{"vacancy_rate"}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{vacancy_type =}
      \KeywordTok{str_to_upper}\NormalTok{(}\KeywordTok{str_replace_all}\NormalTok{(}\KeywordTok{str_remove}\NormalTok{(vacancy_type, }\StringTok{"^prop_"}\NormalTok{), }\StringTok{"_"}\NormalTok{, }\StringTok{" "}\NormalTok{))}
\NormalTok{  ) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\KeywordTok{all_of}\NormalTok{(outcome_vars), vacancy_type, vacancy_rate) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(above_median_cust_affected, vacancy_type) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{prop_vacancy_mean =} \KeywordTok{mean}\NormalTok{(vacancy_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}
    \KeywordTok{aes}\NormalTok{(}
      \DataTypeTok{x =} \KeywordTok{as_factor}\NormalTok{(above_median_cust_affected),}
      \DataTypeTok{y =}\NormalTok{ prop_vacancy_mean,}
      \DataTypeTok{fill =}\NormalTok{ vacancy_type}
\NormalTok{    )}
\NormalTok{  ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{project-writeup_files/figure-latex/unnamed-chunk-62-1.pdf}

\end{document}
